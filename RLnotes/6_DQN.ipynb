{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b64c91",
   "metadata": {},
   "source": [
    "# DQN\n",
    "## I. 用function approximation做TD learning of $V^{\\pi}(S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7434ea",
   "metadata": {},
   "source": [
    "### I.1 目标函数\n",
    "$$J(w) = E\\left [\\left ( v^{\\pi}(S)-\\hat{v}(S,w)\\right )^2 \\right ]$$\n",
    "$v^{\\pi}(S)$和$\\hat{v}(S,w)$分别是给定策略$\\pi$时，所有state value的真实值和估计值。\\\n",
    "目标是找到参数w，满足：\n",
    "$$\n",
    "\\begin{align}\n",
    "w & = \\underset{w}{argmin}J(w) \\\\\n",
    "  & = \\underset{w}{argmin} E\\left [\\left ( v^{\\pi}(S)-\\hat{v}(S,w)\\right )^2 \\right ]\n",
    "\\end{align}\n",
    "$$\n",
    "这里的期望是针对状态$s\\in S$的分布而言。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8406bd0e",
   "metadata": {},
   "source": [
    "### I.2 状态分布\n",
    "#### I.2.1 状态转移概率矩阵\n",
    "1. 定义状态转移概率矩阵: 假设每一步都有n种状态。  \\\n",
    "(1)单步状态转移概率矩阵：$P^{\\pi}\\in \\mathbb{R}^{n\\times n} $，其中单个元素为$[P^{\\pi }]_{ij}$，表示当前状态是$s_i$时，下一步状态为$s_j$的概率。\n",
    "$$[P^{\\pi }]_{ij}=p^{\\pi}(s_j|s_i)=\\sum_{a\\in \\mathcal{A}}p(s_j|s_i, a)\\pi(a|s_i)$$\n",
    "(2)k步状态转移概率矩阵：$P^{\\pi,(k)}\\in \\mathbb{R}^{n\\times n}$，简记为$P^{(k)}$，其中单个元素$[P^{\\pi,(k)}]_{ij}$（简记为$[P^{(k)}]_{ij}$）表示在策略$\\pi$下，状态从$s_i$经过k步转移到$s_j$的概率。\n",
    "$$\n",
    "\\begin{align}\n",
    "[P^{\\pi,(k)}]_{ij} & = [P^{(k)}]_{ij} = p^{\\pi}(S_{t_k}=s_j|S_{t_0}=s_i) \\\\\n",
    "[P^{\\pi }]_{ij} & = [P^{(1)}]_{ij}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db795ac6",
   "metadata": {},
   "source": [
    "2. **多步状态转移的概率可以表示为多个单步状态转移概率矩阵相乘。** \\\n",
    "根据经定义，两步时有：\n",
    "$$\n",
    "[(P^{\\pi})^2]_{i, j}  = [P^{\\pi}\\times P^{\\pi}]_{i, j}  = \\sum_{h=1}^n [P^{\\pi}]_{ih}\\times [P^{\\pi}]_{hj} = [P^{(2)}]_{i,j} \n",
    "$$\n",
    "三步时有：\n",
    "$$\n",
    "\\begin{align}\n",
    "[(P^{\\pi})^3]_{i, j} \n",
    "& = [(P^{\\pi})^2\\times P^{\\pi}]_{i, j} \\\\\n",
    "& = \\sum_{h_2=1}^n \\underset{p(s_i\\rightarrow\\forall s\\rightarrow s_{h2})}{\\underbrace{[(P^{\\pi})^2]_{ih_2}}}\\times [P^{\\pi}]_{h_2j}  \\\\\n",
    "& = \\sum_{h_2=1}^n \\underset{p(2步从s_i到s_{h2})}{\\underbrace{[P^{(2)}]_{ih_2}}}\\times  \\underset{p(s_{h2}\\rightarrow s_j)}{\\underbrace{[P^{\\pi}]_{h_2j}}}  \\\\\n",
    "& =  \\underset{p(3步从s_i\\rightarrow s_j)}{\\underbrace{[P^{(3)}]_{ij}}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "扩展到多步：\n",
    "$$\n",
    "[(P^{\\pi})^k]=[\\underset{k个P^{\\pi}相乘}{\\underbrace{P^{\\pi}\\times P^{\\pi}\\times ... \\times P^{\\pi}}}  ]_{i, j}\n",
    "= [P^{(k)}]_{i,j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c6aabe",
   "metadata": {},
   "source": [
    "#### I.2.2 定义状态的分布\n",
    "1. $d_0\\in \\mathbb{R}^n$是n维向量，第i个元素$d_0(s_i)$表示起点时刻处于状态$s_i$的概率。\n",
    "2. $d_k(s_i)$表示从起点开始，经过k步之后，agent处于状态$s_i$的概率。\n",
    "$$\n",
    "\\begin{align}\n",
    "d_k(s_i) & = \\sum _{h = 1}^nd_0(s_h)[P^{(k)}]_{hi} \\ ,\\ i = 1, 2, ..., n \\\\\n",
    "矩阵表达方式： \\\\\n",
    "d_k^T & = d_0^TP^{(k)}\n",
    "\\end{align}$$\n",
    "3. 状态分布的变化取决于状态转移概率\n",
    "$$\n",
    "d_k^T=d_{k-1}^TP^{(1)}=d_{k-1}^TP^{\\pi}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6869e0ce",
   "metadata": {},
   "source": [
    "### I.3 状态分布的收敛\n",
    "#### I.3.1 符号\n",
    "$d^{\\pi}(s)$表示agent用策略$\\pi$经过很长一段时间后到达状态s的概率。\n",
    "$$\n",
    "d^{\\pi}=\\begin{pmatrix}\n",
    "d^{\\pi}(s_1) \\\\\n",
    "d^{\\pi}(s_2) \\\\\n",
    "... \\\\\n",
    "d^{\\pi}(s_n)\n",
    "\\end{pmatrix}, 且\\sum_{s\\in S}d^{\\pi}(s)=1\n",
    "$$\n",
    "#### I.3.2 两种收敛结果\n",
    "1. **stationary distribution** \\\n",
    "stationary distribution刻画了马尔科夫过程的long-term behavior。在满足一定条件的情况下，当agent使用策略$\\pi$与环境互动了sufficiently long period之后，state distribution $d_k$会收敛到constant value $d^{\\pi}$。如果满足：\n",
    "$$\\therefore (d^{\\pi})^T = (d^{\\pi})^TP^{\\pi }$$\n",
    "就将$d^{\\pi}$称为stationarity distribution。\n",
    "2. **limiting distribution** \\\n",
    "如果满足：\n",
    "$$\\lim_{k \\to \\infty} d_k^T =(d^{\\pi})^T$$\n",
    "就将$d^{\\pi}$称为limiting distribution。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c1e56",
   "metadata": {},
   "source": [
    "3. <font color=red>两种收敛结果的区别：如果分布满足limiting distribution，则也就满足stationary distribution。但反之不然。</font> \\\n",
    "(1) 证明limiting distribution必然满足stationary distribution\n",
    "<font color=grey>\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\because d_k^T  = d_{k-1}^TP^{\\pi} \\\\\n",
    "&\\therefore \\lim_{k \\to \\infty} d_k^T = \\lim_{k \\to \\infty} d_{k-1}^TP^{\\pi} \\\\\n",
    "& 根据定义，如果是limiting\\ distribution，则：\\\\\n",
    "&\\lim_{k \\to \\infty} d_k^T = (d^{\\pi})^T,\\lim_{k \\to \\infty} d_{k-1}^T= (d^{\\pi})^T \\\\\n",
    "&代入上式有： (d^{\\pi})^T = (d^{\\pi})^TP^{\\pi } \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "</font>\n",
    "(2) stationary distribution也是limiting distribution的条件是：存在某个时刻k的状态分布能达到$d^{\\pi}$，那么在k时刻之后的迭代必然一直停在$d^{\\pi}$上。否则，虽然存在$d^{\\pi}$满足$(d^{\\pi})^T=(d^{\\pi})^TP^{\\pi}$，但不能保证agent一定能实现这样的状态分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2736fb7d",
   "metadata": {},
   "source": [
    "#### I.3.3 状态分布收敛到limiting distribution\n",
    "1. limiting distribution的<font color=blue>**充分条件**</font>：$\\lim_{k \\to \\infty} (P^{\\pi })^k = \\mathbb{1}_n(d^{\\pi})^T $ \\\n",
    "分析：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\lim_{k \\to \\infty} (P^{\\pi })^k & = \\mathbb{1}_n(d^{\\pi})^T \\\\\n",
    "&= \\begin{pmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "... \\\\\n",
    "1\n",
    "\\end{pmatrix}(d^{\\pi}(s_1), d^{\\pi}(s_2),... ,d^{\\pi}(s_n))\\\\\n",
    "&=\\begin{pmatrix}\n",
    "d^{\\pi}(s_1)  & d^{\\pi}(s_2) & ... & d^{\\pi}(s_n)\\\\\n",
    "d^{\\pi}(s_1)  & d^{\\pi}(s_2) & ... & d^{\\pi}(s_n)\\\\\n",
    "...  &  &  & \\\\\n",
    "d^{\\pi}(s_1)  & d^{\\pi}(s_2) & ... & d^{\\pi}(s_n)\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "充分性证明：\\\n",
    "根据状态分布的定义，有：\n",
    "$$\n",
    "\\lim_{k \\to \\infty} d_k^T =d_0^T\\lim_{k \\to \\infty} (P^{\\pi })^k= d_0^T\\mathbb{1}_n(d^{\\pi})^T = (d^{\\pi})^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8073a2",
   "metadata": {},
   "source": [
    "2. <font color=green>**能否收敛到limiting distribution，以及收敛后$d^{\\pi}$的取值由状态转移概率矩阵$P^{\\pi}$决定，与agent的初始状态无关。**</font> \\\n",
    "<font color=blue>注：状态转移概率矩阵$P^{\\pi}$是由transition function和policy function决定的。</font> \\\n",
    "证明：\n",
    "$$\n",
    "\\begin{align}\n",
    "如果状态分布收敛： \\\\\n",
    "&\\because d_k^T  = d_{k-1}^TP^{\\pi} \\\\\n",
    "&\\therefore \\lim_{k \\to \\infty} d_k^T = \\lim_{k \\to \\infty} d_{k-1}^TP^{\\pi}= \\left (\\lim_{k \\to \\infty} d_{k-1}^T\\right )P^{\\pi} \\\\\n",
    "&\\therefore (d^{\\pi})^T = (d^{\\pi})^TP^{\\pi } \\\\\n",
    "&\\therefore 0 = (d^{\\pi})^T(P^{\\pi }-I) \\\\\n",
    "等价于求解： \\\\\n",
    "&(P^{\\pi }-I)^Td^{\\pi} = 0 \\\\\n",
    "& \\left [(P^{\\pi})^T-I \\right]d^{\\pi} = 0 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "所以$d^{\\pi}$是矩阵$(P^{\\pi})^T$的特征根，且对应的特征向量是1。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b807088d",
   "metadata": {},
   "source": [
    "#### I.3.4 收敛条件：<font color=blue>irreducible or regular Markov process能够收敛到唯一的limiting distribution。</font>\n",
    "1. 相关概念 \\\n",
    "(1) **accessible**: 如果$[P^{\\pi}]^k_{ij}>0，且k是有限的，$则称从状态si到状态sj是accessible的。\\\n",
    "(2) **communicate**: 如果从状态si到状态sj是accessible的，且反过来从sj到si也是accessible的，那么就说si和sj是communicate的。\\\n",
    "(3) **irreducible**: 如果所有的状态pair都是communicate的，那么就称该马尔科夫过程是irreducible的。从数学定义上看，也就是对任意i，j，都存在$k\\ge 1$使得$[P^k]_{ij}>0$ \\\n",
    "(4) **regular**: 如果存在$k\\ge 1$使得对任意i，j都满足$[P^k]_{ij}>0$，则称该马尔科夫过程是regular的。可以简单表述为：存在k，使得$P^{\\pi,(k)}>0$\\\n",
    "<font color=red>**注意irreducible和regular的区别**：irreducible只要求对于任意$s_i和s_j$，都能找到一个k，满足$[P^k]_{ij}>0$。因此每个状态pair找到的k值可以不同。regular要求存在同一个k，对任意$s_i和s_j$都满足$[P^k]_{ij}>0$。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3658c473",
   "metadata": {},
   "source": [
    "2. regular和irreducible相关的性质 \\\n",
    "(1) 如果一个马尔科夫过程是regular的，那么它必然是irreducible的。(显而易见) \\\n",
    "(2) 如果一个马尔科夫过程是irreducible的，且存在状态$s_i$满足$[p^{\\pi}]_{ii}>0$，那么该马尔科夫过程就是regular的。\\\n",
    "（证略）\\\n",
    "(3) 如果$P^{\\pi,(k)}>0$，那么对$\\forall k^{'}> k$都满足$P^{\\pi,(k^{'})}>0$。\\\n",
    "<font color=grey>简单证明：因为马尔科夫过程满足regular时，$P^{\\pi}$不可能有全为0的列向量。</font> \\\n",
    "(4) 当马尔科夫过程是regular的，如果$\\lim_{k \\to \\infty} (P^{\\pi })^k = \\mathbb{1}_n(d^{\\pi})^T$，则对所有状态s都有$d^{\\pi}(s)>0$。 \\\n",
    "<font color=grey>证明：\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\because \\lim_{k \\to \\infty} (P^{\\pi })^k = \\mathbb{1}_n(d^{\\pi})^T \\\\\n",
    "& \\therefore \\lim_{k \\to \\infty} d_k^T = d_0^T\\lim_{k \\to \\infty}(P^{\\pi})^k = d_0^T\\mathbb{1}_n(d^{\\pi})^T= (d^{\\pi})^T \\\\\n",
    "& \\because (P^{\\pi})^k > 0 , 且d_0不会所有元素都为0\\\\\n",
    "& \\therefore (d^{\\pi})^T > 0 \n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec110428",
   "metadata": {},
   "source": [
    "4. **什么样的策略能实现regular Markov process** \\\n",
    "<font color=blue>一般而言，exploratory policies比如$\\epsilon-greedy$策略能够让马尔科夫过程具备regular特点。</font>\\\n",
    "说明：因为exploratory policies在任意时刻选择任意状态的概率都大于0。只要transition function允许，那么马尔科夫过程的所有状态pair都是communicate的。也因此，该马尔科夫过程是irreducible的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d0cad",
   "metadata": {},
   "source": [
    "#### I.3.5 状态分布收敛条件下的目标函数\n",
    "$$\n",
    "\\begin{align}\n",
    "J(w) \n",
    "& = E\\left [\\left ( v^{\\pi}(S)-\\hat{v}(S,w) \\right )^2 \\right ] \\\\\n",
    "& = \\sum _{s\\in \\mathcal{S}}d^{\\pi}(s)[v^{\\pi}(s)-\\hat{v}(s,w)]^2\n",
    "\\end{align}\n",
    "$$\n",
    "<font color=orange>注：后文方法并没有显示求解$d^{\\pi}$，因为分布信息已经隐藏在样本中，用好样本的抽样方法即可。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ba07b",
   "metadata": {},
   "source": [
    "### I.4 目标函数的优化\n",
    "#### I.4.1 优化思路\n",
    "1. 目标函数分析：\\\n",
    "(1) 目标函数:\n",
    "$$J(w) = \\frac{1}{2} E\\left [\\left ( v^{\\pi}(S)-\\hat{v}(S,w)\\right )^2 \\right ] $$\n",
    "(2) 梯度：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla J(w) \n",
    "& = \\frac{1}{2} \\nabla_w E\\left [\\left ( v^{\\pi}(S)-\\hat{v}(S,w)\\right )^2 \\right ] \\\\\n",
    "& = -E\\left [v^{\\pi}(S)-\\hat{v}(S,w)\\right  ]\\nabla_w \\hat{v}(S,w)\n",
    "\\end{align}\n",
    "$$\n",
    "(3) 标准gradient descent算法：\n",
    "$$\n",
    "\\begin{align}\n",
    "w_{k+1} &  = w_k - \\alpha_k\\nabla_w J(w) \\\\\n",
    "&= w_k + \\alpha_kE\\left [v^{\\pi}(S)-\\hat{v}(S,w)\\right  ]\\nabla_w \\hat{v}(S,w)\n",
    "\\end{align}\n",
    "$$\n",
    "(4) SGD算法：\n",
    "$$\n",
    "\\begin{align}\n",
    "w_{t+1} &  = w_t - \\alpha_t\\nabla_w J(w_t) \\\\\n",
    "&= w_t + \\alpha_t\\left [v^{\\pi}(s_t)-\\hat{v}(s_t,w_t)\\right  ]\\nabla_w \\hat{v}(s_t,w_t)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d204f89",
   "metadata": {},
   "source": [
    "2. SGD的应用限制和解决方式 \\\n",
    "(1) **<font color=red>算法应用限制：由于$v^{\\pi}(s_t)$未知，因此无法直接使用SGD。</font>** \\\n",
    "(2) 两种解决办法：\\\n",
    "a. Monte Carol方法：用episode的信息来估计所有状态$s$的state value，然后代入SGD求解。\\\n",
    "b. TD 方法：\\\n",
    "迭代式为：\n",
    "$$\n",
    "\\color{Blue} {w_{t+1} = w_t + \\alpha_t\\left [r_{t+1}+\\gamma \\hat{v}(s_{t+1,w_t})-\\hat{v}(s_t,w_t)\\right  ]\\nabla_w \\hat{v}(s_t,w_t)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e71b3f6",
   "metadata": {},
   "source": [
    "#### I.4.2 TD learning的收敛性分析：简化以linear function approximation为例\n",
    "1. 代入线性参数后的迭代式：\n",
    "假设：$v(s,w)$是w的线性函数，$v(s, w)=\\phi^T(s)w$\n",
    "$$ w_{t+1} = w_t + \\alpha_t\\left [r_{t+1}+\\gamma \\phi^T(s_{t+1})w_t-\\phi^T(s_t)w_t\\right  ]\\phi^T(s_t) $$\n",
    "2. 迭代式的收敛性质 \\\n",
    "(1) 该迭代式能收敛到唯一最优解$w^*$ \\\n",
    "(2) 收敛到的最优解满足：\n",
    "$$\n",
    "\\begin{align}\n",
    "w^* \n",
    "& = \\underset{w}{argmin}  \\ J_{BE}(w) \\\\\n",
    "& = \\underset{w}{argmin} \\left \\| \\hat v(w)-(r^{\\pi}+\\gamma P^{\\pi}\\hat v(w)) \\right \\| ^2_D \\\\\n",
    "其中&，\\left \\| x \\right \\|_D = x^TDx\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3e9ff1",
   "metadata": {},
   "source": [
    "3. 收敛性质的证明思路：\\\n",
    "<font color=green>构造中间方程，该方程满足性质：迭代式只要能让中间方程收敛，就能让原方程收敛。</font> \\\n",
    "**构造的中间方程为：**\n",
    "$$\n",
    "w_{t+1} = w_t + \\alpha_tE\\left [\\left(r_{t+1}+\\gamma \\phi^T(s_{t+1})w_t-\\phi^T(s_t)w_t\\right )\\phi^T(s_t)\\right ]\n",
    "$$\n",
    "(1) 方程中的期望是对随机变量$s_t, s_{t+1}, r_{t+1}$而言。但整个方程是确定的，因为随机性在求期望之后消失。 \\\n",
    "(2) <font color=red>TD-linear的迭代式是该方程的SGD形态。所以，如果该方程收敛，那么TD-linear的迭代也是收敛的。</font>[这里没有证明，只是给出了直觉。]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ca15da",
   "metadata": {},
   "source": [
    "4. 分析中间方程的收敛性质 \\\n",
    "(1) 中间方程重构 \\\n",
    "$$\n",
    "\\begin{align}\n",
    "记D=\\begin{pmatrix}\n",
    "d^{\\pi}(s_1) & & & & 0\\\\\n",
    "  &  ... & & & \\\\\n",
    "&  & d^{\\pi}(s_k) & & \\\\\n",
    " & &  &... & \\\\\n",
    "0 & &  & & d^{\\pi}(s_n)\n",
    "\\end{pmatrix}, \\Phi = \\begin{bmatrix}\n",
    "\\phi^T(s_1) \\\\\n",
    "... \\\\\n",
    "\\phi^T(s_k) \\\\\n",
    "... \\\\\n",
    "\\phi^T(s_n)\n",
    "\\end{bmatrix} \\\\\n",
    "\\end{align} \\\\\n",
    "取b = \\Phi^TDr^{\\pi } \\in \\mathbb{R}^{m\\times m}，A = \\Phi^TD(I-\\gamma P^{\\pi})\\Phi \\in \\mathbb{R}^{m}\n",
    "$$代入中间方程的期望部分有：\n",
    "$$E\\left [\\left(r_{t+1}+\\gamma \\phi^T(s_{t+1})w_t-\\phi^T(s_t)w_t\\right )\\phi^T(s_t)\\right ] = b - Aw_t$$\n",
    "中间方程重构为：\n",
    "$$w_{t+1}=w_t+\\alpha_t(b-Aw_t)$$\n",
    "(2) 重构后的方程满足RM算法条件，可以证明：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\lim_{t \\to \\infty}w_t\\rightarrow w^*=A^{-1}b\n",
    "\\end{align}\n",
    "$$\n",
    "<font color=brown>[证明见math of RL page179 proof2]</font> \\\n",
    "(3) 相关性质：可以证明A可逆，且是正定矩阵。 [证略]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc4bbf",
   "metadata": {},
   "source": [
    "5. 收敛结果：用迭代法解中间方程得到的$w^*$可以最小化Bellman error。即下述目标函数的解：\n",
    "$$\n",
    "\\begin{align}\n",
    "J_{BE}(w) & =\\left \\| \\hat v(w)-(r^{\\pi}+\\gamma P^{\\pi}\\hat v(w)) \\right \\| ^2_D ，其中，\\left \\| x \\right \\|_D = x^TDx\\\\\n",
    "w^* & = \\underset{w}{argmin}  \\ J_{BE}(w)\n",
    "\\end{align}\n",
    "$$\n",
    "[证略]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6275ca9e",
   "metadata": {},
   "source": [
    "## II. 用function approximation做TD learning of $Q^{\\pi}(S,A)$\n",
    "### II.1 Sarsa with function approximation\n",
    "#### II.1.1 算法思想\n",
    "思想和TD learning of V(S)一样，迭代式改为：\n",
    "$$\n",
    "\\color{Blue} {w_{t+1} = w_t + \\alpha_t\\left [r_{t+1}+\\gamma \\hat{q}(s_{t+1,a_{t+1},w_t})-\\hat{q}(s_t,a_t,w_t)\\right  ]\\nabla_w \\hat{q}(s_t,a_t,w_t)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a579ff8",
   "metadata": {},
   "source": [
    "#### II.1.2 算法伪码\n",
    "<img src=\"./pics/sarsaapprox.png\" alt=\"alt text\" width=\"560\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e4bce9",
   "metadata": {},
   "source": [
    "### II.2 Q-learning with function approximation\n",
    "#### II.1.1 算法思想\n",
    "思想和TD learning of V(S)一样，迭代式改为：\n",
    "$$\n",
    "\\color{Blue} {w_{t+1} = w_t + \\alpha_t\\left [r_{t+1}+\\gamma\\underset{a\\in \\mathcal{A}}{max}\\ \\hat{q}(s_{t+1},a,w_t)-\\hat{q}(s_t,a_t,w_t)\\right  ]\\nabla_w \\hat{q}(s_t,a_t,w_t)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd53c9",
   "metadata": {},
   "source": [
    "#### II.2.2 算法伪码\n",
    "<img src=\"./pics/qlearningapprox.png\" alt=\"alt text\" width=\"560\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b483597",
   "metadata": {},
   "source": [
    "## III. Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9877b79",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fec9b7e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f91eac5d",
   "metadata": {},
   "source": [
    "<img src=\"./pics/deepqlearning.png\" alt=\"alt text\" width=\"560\"/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
