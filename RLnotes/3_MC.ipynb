{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ee6dad",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1858a8",
   "metadata": {},
   "source": [
    "### I. 最简单的MC-based 算法：direct learning\n",
    "#### I.1 思路\n",
    "1. 根据给定策略条件下的value function定义：$$\\begin{align} \n",
    "V^{\\pi}(s)& =E(U|\\pi,s_0=s)\\\\\n",
    "& =E\\left [ \\sum_{t=0}^{\\infty } \\gamma ^tR(s_t, \\pi(s_t), s_{t+1}) |s_0=s\\right ] \\\\\n",
    "\\end{align}$$可以直接用抽样方法找不同states的$V^{\\pi}(s)$估计值。\n",
    "2. 方法：固定策略$\\pi$，直接采样$V^{\\pi}(s)$。1个以$s_i$为起点的episode得到1个$V^{\\pi}(s_i)$的样本。用均值作为$V^{\\pi}(s_i)$的估计值。\n",
    "$$\\begin{align} \n",
    "& sample_1 = V^{\\pi}(s_i)=R(s,\\pi(s),s^{'}_1)+\\sum_{t=1}^{terminal } \\gamma ^tR(s_{1,t}, \\pi(s_{1,t}), s_{1,t+1})\\\\\n",
    "& sample_2 = V^{\\pi}(s_i)=R(s,\\pi(s),s^{'}_2)+\\sum_{t=1}^{terminal } \\gamma ^tR(s_{2,t}, \\pi(s_{2,t}), s_{2,t+1})\\\\\n",
    "& ...\\\\\n",
    "& sample_n = V^{\\pi}(s_i)=R(s,\\pi(s),s^{'}_n)+\\sum_{t=1}^{terminal } \\gamma ^tR(s_{n,t}, \\pi(s_{n,t}), s_{n,t+1})\\\\\n",
    "& V^{\\pi }(s_i) \\leftarrow \\frac{1}{n}\\sum_{j}sample_j \n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a188595c",
   "metadata": {},
   "source": [
    "#### I.2 算法\n",
    "<img src=\"./pics/MC_basics.png\" alt=\"alt text\" width=\"560\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cecced",
   "metadata": {},
   "source": [
    "#### I.3 应用要点\n",
    "1. 问题：信息浪费，导致准确估计Value function所需要的episode量很大。因为1个以$s_i$为起点的episode只采纳对应的total rewards作为有效信息。这个episode在exploration过程中rewards value包含了不同的states之间transition的关联信息，都没有被用到。这也导致准确估计Value function所需要的episode量很大。\n",
    "2. **用法**：<font color=green>direct learning（包括后面的temporal difference learning）只是找$V^{\\pi}$的方法。但原本的目标是找到最优策略。所以找到$V^{\\pi}$之后，通常需要再结合model-based learning方法来获得transition function和rewards function的信息。\\\n",
    "有了$V^{\\pi}(s),T(s, \\pi(s), s^{'}),R(s, \\pi(s), s^{'})$的信息后，再用$$Q^*(s, a)=\\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{*}(s^{'})]$$来迭代最优策略。\\\n",
    "所以direct learning和TD learning相当于完成了用policy iteration求解MDP问题时的policy evaluation的那一步，而policy improvement/extraction的那一步还需要额外的model-based learning的方法来做。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163523d4",
   "metadata": {},
   "source": [
    "### II. MC exploring starts\n",
    "#### II.1 更高效地使用样本\n",
    "1. 思路：direct learning方法只用了total rewards value，没有使用中间状态的reward信息，因此造成了信息浪费，一种解决这个问题的思路是，利用中间信息来做policy evaluation。<font color=red>不再用episode为抽样的单位，而是用一次action-reward为抽样的单位。</font>\\\n",
    "· <font color=blue>**policy evaluation**</font>：给定策略$\\pi$，可以用迭代法求解value/utility:$$\\begin{align} \n",
    "& V^{\\pi}_0(s)=0\\\\\n",
    "& iterate:V^{\\pi}_{k+1}(s)\\leftarrow \\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,\\pi(s),s^{'})+\\gamma V^{\\pi}_{k}(s^{'})]\\\\\n",
    "& until:V^{\\pi}(s) \\ stable\n",
    "\\end{align}$$\n",
    "在没有T和R信息的情况下，用抽样的方法求解。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72f0447",
   "metadata": {},
   "source": [
    "2. 方法： 从$V^{\\pi}_0(s)=0$开始按下述方式抽样，直到收敛。\n",
    "$$\\begin{align} \n",
    "第1轮：\n",
    "& for\\ s\\ in\\ S:\\\\\n",
    "&sample_1 = R(s, \\pi(s), s^{'}_1)+\\gamma V^{\\pi}_0(s^{'}_1) \\\\\n",
    "&sample_2 = R(s, \\pi(s), s^{'}_2)+\\gamma V^{\\pi}_0(s^{'}_2) \\\\\n",
    "&... \\\\\n",
    "&sample_n = R(s, \\pi(s), s^{'}_n)+\\gamma V^{\\pi}_0(s^{'}_n) \\\\\n",
    "&V^{\\pi }_{1}(s) \\leftarrow \\frac{1}{n}\\sum_{i}sample_i \\\\\n",
    "...\\\\\n",
    "第k轮：& for\\ s\\ in\\ S:\\\\\n",
    "& sample_1 = R(s, \\pi(s), s^{'}_1)+\\gamma V^{\\pi}_k(s^{'}_1) \\\\\n",
    "& sample_2 = R(s, \\pi(s), s^{'}_2)+\\gamma V^{\\pi}_k(s^{'}_2)\\\\\n",
    "& ...\\\\\n",
    "& sample_n = R(s, \\pi(s), s^{'}_n)+\\gamma V^{\\pi}_k(s^{'}_n) \\\\\n",
    "& V^{\\pi }_{k}(s) \\leftarrow \\frac{1}{n}\\sum_{i}sample_i  \\\\\n",
    "直到收敛\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49a66d",
   "metadata": {},
   "source": [
    "3. 应用要点 \\\n",
    "(1) 问题：为了求$V^{\\pi}(s)$，需要每次都从指定的time horizon，指定的状态s开始抽样，但实际环境中很可能难以精确控制这些因素。比如，状态难以复现的情况很常见。\\\n",
    "(2) <font color=green>**[rk's note]改进的思路**：上面的抽样方式限制很大。实际上agent在环境中做action的时候，最方便的是按照既定策略$\\pi$和start state做完整个episode。如果能将这样的episode中每个action-reward的信息作为样本使用，那实现起来会方便很多。</font><font color=red>TD learning就是这样的思路。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353624f6",
   "metadata": {},
   "source": [
    "#### II.2 更高效地策略迭代\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f49cc",
   "metadata": {},
   "source": [
    "#### II.3 算法伪码\n",
    "<img src=\"./pics/MC_exploring.png\" alt=\"alt text\" width=\"560\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71452e6e",
   "metadata": {},
   "source": [
    "### III. MC $\\epsilon$-greedy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c937c5",
   "metadata": {},
   "source": [
    "#### III.1 思路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9de876",
   "metadata": {},
   "source": [
    "#### II.2 算法伪码\n",
    "<img src=\"./pics/e_greedy.png\" alt=\"alt text\" width=\"560\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
