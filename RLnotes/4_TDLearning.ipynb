{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2558313c",
   "metadata": {},
   "source": [
    "# TD Learning\n",
    "## I. reinforcement learning的基本概念\n",
    "1. online planning和offline planning \\\n",
    "① **offline planning** \\\n",
    "知道transition function和rewards function，可以直接求最优策略。MDP就是典型的offline planning。\\\n",
    "② **online planning** \\\n",
    "不知道transition function和rewards function，无法直接求最优策略。agent需要通过exploration来获取环境信息。\n",
    "\n",
    "2. sample and episode \\\n",
    "① **sample**: 在online planning中，agent的一次行为反馈过程$(s, a, s^{'}, r)$称为一个sample。 \\\n",
    "② **episode**: 在online planning中，agent持续take action，并collect samples，直到达到terminal state的整个过程称为一个episode。通常要经历多个episods才能完成exploration。\n",
    "\n",
    "3. model-based learning and model-free learning \\\n",
    "① <font color=blue>**model-based learning**</font>：先利用exploration得到的episodes数据，来构造model估计transition function和rewards function，然后就可以将问题转化为MDP问题求解。\\\n",
    "② <font color=blue>**model-free learning**</font>：不估计transition function和rewards function，而是直接估计value或者Q-values of states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ca9dc",
   "metadata": {},
   "source": [
    "## II. model-based learning\n",
    "1. 估计方法 \\\n",
    "① 估计transition function：$\\hat T(s, a, s^{'})=\\frac{\\#(s, a, s^{'})}{\\#(s, a)}$ \\\n",
    "② 估计reward function：$\\hat R(s, a, s^{'})= R(s, a, s^{'})$\n",
    "2. 收敛条件：根据大数定律，增加样本量可以让估计趋于真实值。\n",
    "3. 应用场景：保留所有可能的$(s, a, s^{'})$table需要的存储成本很大，所以model-based learning多用在存储能力足够的场景下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ada77",
   "metadata": {},
   "source": [
    "## III. model-free learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec67dc4",
   "metadata": {},
   "source": [
    "### III.1 passive reinforcement learning\n",
    "在passive reinforcement learning中，会先给agent一个policy，然后agent用这个policy来经历episodes，以learn环境中的状态信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8301c36",
   "metadata": {},
   "source": [
    "#### III.1.3 temporal difference learning (TD learning)\n",
    "1. **思路**：\\\n",
    "① 和前面基于direct learning改进的方法很相似，也是在不知道Transition和Reward函数的条件下，将每一步中间action-reward作为一个样本，用抽样的方法来估计policy evaluation的结果。\n",
    "$$V^{\\pi}_{k+1}(s)\\leftarrow \\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,\\pi(s),s^{'})+\\gamma V^{\\pi}_{k}(s^{'})]$$\n",
    "② 但是改变了如何使用抽样sample来计算$V^{\\pi}(s)$的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9b619",
   "metadata": {},
   "source": [
    "2. 方法：\n",
    "$$\\begin{align} \n",
    "& V^{\\pi }(s)_0=0 \\\\\n",
    "iterate: \\\\\n",
    "& nth\\ sample\\ of\\  V^{\\pi}(s): \\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ sample_n = R(s,\\pi(s),s^{'})+\\gamma V^{\\pi }(s^{'})\\\\\n",
    "& V^{\\pi }(s)_{n} \\leftarrow (1-\\alpha)*V^{\\pi }(s)_{n-1}+\\alpha* sample_n\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27dd49",
   "metadata": {},
   "source": [
    "3. 为什么可以这样迭代 \\\n",
    "详见[动手学强化学习教材p48]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae08c1c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb924358",
   "metadata": {},
   "source": [
    "### III.2 active reinforcement learning\n",
    "在active reinforcement learning中，没有固定的policy，agent会随时根据learn到的环境信息来更新policy。也就是exploration和exploiting同时进行。\n",
    "#### III.2.1 Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec1250",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bc24796",
   "metadata": {},
   "source": [
    "### III.4 Approximate Q-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
