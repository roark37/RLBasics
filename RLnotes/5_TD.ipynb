{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2558313c",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning\n",
    "## I. reinforcement learning的基本概念\n",
    "1. online planning和offline planning \\\n",
    "① **offline planning** \\\n",
    "知道transition function和rewards function，可以直接求最优策略。MDP就是典型的offline planning。\\\n",
    "② **online planning** \\\n",
    "不知道transition function和rewards function，无法直接求最优策略。agent需要通过exploration来获取环境信息。\n",
    "\n",
    "2. sample and episode \\\n",
    "① **sample**: 在online planning中，agent的一次行为反馈过程$(s, a, s^{'}, r)$称为一个sample。 \\\n",
    "② **episode**: 在online planning中，agent持续take action，并collect samples，直到达到terminal state的整个过程称为一个episode。通常要经历多个episods才能完成exploration。\n",
    "\n",
    "3. model-based learning and model-free learning \\\n",
    "① <font color=blue>**model-based learning**</font>：先利用exploration得到的episodes数据，来构造model估计transition function和rewards function，然后就可以将问题转化为MDP问题求解。\\\n",
    "a. 估计transition function：$\\hat T(s, a, s^{'})=\\frac{\\#(s, a, s^{'})}{\\#(s, a)}$ \\\n",
    "b. 估计reward function：$\\hat R(s, a, s^{'})= R(s, a, s^{'})$ \\\n",
    "② <font color=blue>**model-free learning**</font>：不估计transition function和rewards function，而是直接估计value或者Q-values of states.比如MC和TD。\\\n",
    "应用场景来看：model based model保留所有可能的$(s, a, s^{'})$table需要的存储成本很大，所以model-based learning多用在存储能力足够的场景下。\n",
    "4. passive RL和active RL\n",
    "① 在passive reinforcement learning中，会先给agent一个policy，然后agent用这个policy来经历episodes，以learn环境中的状态信息。TD就是passive RL方法。 \\\n",
    "② 在active reinforcement learning中，没有固定的policy，agent会随时根据learn到的环境信息来更新policy。也就是exploration和exploiting同时进行。Q-learning就是active RL方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec67dc4",
   "metadata": {},
   "source": [
    "## II.basic TD：TD learning of state value\n",
    "### II.1 思路：用MC抽样+RM迭代计算$V^{\\pi}(s)$，<font color=red>做policy evaluation</font>\n",
    "和MC exploring starts方法相似，也是在不知道Transition和Reward函数的条件下，将每一步中间action-reward作为一个样本，用抽样的方法来估$V^{\\pi}(s)$，做policy evaluation。只是改用RM算法的迭代方式来估$V^{\\pi}(s)$。\n",
    "$$V^{\\pi}_{k+1}(s)\\leftarrow \\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,\\pi(s),s^{'})+\\gamma V^{\\pi}_{k}(s^{'})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8301c36",
   "metadata": {},
   "source": [
    "### II.2 算法\n",
    "1. basic TD算法：做policy evaluation\n",
    "$$\n",
    "\\begin{align} \n",
    "& V^{\\pi }(s)_0=0 \\\\\n",
    "iterate: \\\\\n",
    "& nth\\ sample\\ of\\  V^{\\pi}(s): \\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ sample_n = R(s,\\pi(s),s^{'})+\\gamma V^{\\pi }(s^{'})\\\\\n",
    "& V^{\\pi }(s)_{n} \\leftarrow V^{\\pi }(s)_{n-1}-\\alpha (V^{\\pi }(s)_{n-1}-sample_n)\\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = (1-\\alpha)*V^{\\pi }(s)_{n-1}+\\alpha* sample_n\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9b619",
   "metadata": {},
   "source": [
    "2. 用RM推导basic TD的迭代形式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27dd49",
   "metadata": {},
   "source": [
    "3. 为什么可以这样迭代 \\\n",
    "详见[动手学强化学习教材p48]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae08c1c",
   "metadata": {},
   "source": [
    "## III. Sarsa：TD learning of action value\n",
    "### III.1 Sarsa algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa78ca56",
   "metadata": {},
   "source": [
    "### III.2 n-step Sarsa algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb924358",
   "metadata": {},
   "source": [
    "## IV. Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec1250",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bc24796",
   "metadata": {},
   "source": [
    "### III.4 Approximate Q-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
