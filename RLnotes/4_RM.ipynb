{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c29d61d5",
   "metadata": {},
   "source": [
    "# Robins-Monro algorithm\n",
    "RM算法是stochastic approximation算法中的一种。\n",
    "## I. RM算法\n",
    "<font color=green>**本质：$$迭代直到收敛：新估计 = 旧估计 - 学习速度*[旧估计 - 新样本值]$$**</font>\n",
    "### I.1 适用场景\n",
    "1. 求解g(w)=0函数\n",
    "2. 不知道objective function即g(w)的表达式，但是有很多{w，g(w)}的样本pairs。比如一个NN，没有对应的方程表达式，但是有输入和输出样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac25f13",
   "metadata": {},
   "source": [
    "### I.2 算法描述\n",
    "$$迭代求解：w_{k+1}=w_{k}-a_k\\tilde g(w_k, \\eta_k)，k=1, 2, 3, ...$$\n",
    "说明：\\\n",
    "(1) $w_k$是$g(w)=0$函数根的第k次估计 \\\n",
    "(2) $\\tilde g(w_k,\\eta_k)是$g(w)$的第k个样本观测值$。$\\tilde g(w_k, \\eta_k)  = g(w_k) + \\eta$ \\\n",
    "(3) $a_k$是一个positive coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875b15e6",
   "metadata": {},
   "source": [
    "### I.3 收敛条件：当下面三个条件满足时，RM算法迭代保证收敛到$w^*$ \n",
    "**RM theory**\n",
    "1. $0<c_1\\le \\nabla _wg(w)\\le c_2$。<font color=orange>下限不能取0是保证g(w)=0必然有解的关键。</font> \n",
    "2. $\\sum^{\\infty}_{k=1}a_k=\\infty,且\\sum^{\\infty}_{k=1}a_k^2<\\infty$ \\\n",
    "(1) $\\sum^{\\infty}_{k=1}a_k=\\infty$:确保遍历范围达到所有可能的取值，不管初始$w_0$距离$w^*$有多远，都能收敛到最优值。 \\\n",
    "(2) $\\sum^{\\infty}_{k=1}a_k^2<\\infty$:确保当迭代次数足够大，到达最优值附近的时候，$w_k$不要大幅抖动。\\\n",
    "(3) 例子：$a_k=\\frac{1}{k}$。虽然满足上面两个条件，但也有明显的缺点。当迭代次数k越来越大以后，$\\frac{1}{k}$逼近0的速度很快，导致后面的样本对w更新的影响很小，所以不常用。\n",
    "3. $E(\\eta_k)=0, E(\\eta^2_k)<\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029732a",
   "metadata": {},
   "source": [
    "### I.4 从RM alg的角度重新理解：$\\hat E(X)=\\frac{1}{n}\\sum_{i=1}^{n}x_i$\n",
    "<font color=green>**用均值求期望是RM算法的一个典型例子**</font>\n",
    "1. 问题：有样本$\\{x_i\\}, i=1, 2, ...$，求$\\hat E(X)$\n",
    "2. 等价问题：将求解$E(X)=w$转化为求解$g(w)=w-EX=0$。<font color=red>**[注：不能是$g(w)=EX-w=0$，否则不满足RM算法的第一个条件]**</font>\n",
    "3. 分析：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde g(w,\\eta ) & = w-x\\\\\n",
    "& = w - EX + EX - x\\\\\n",
    "& = (w - EX) + (EX - x)\\\\\n",
    "&取\\eta =EX-x \\\\\n",
    "\\tilde g(w,\\eta ) & = g(w) + \\eta \n",
    "\\end{align}\n",
    "$$\n",
    "4. 构造迭代算法\n",
    "$$\n",
    "\\begin{align}\n",
    "w_{k+1} & = w_{k}-a_k\\tilde g(w_k, \\eta_k) \\\\\n",
    "& = w_{k}-a_k(w_k-x_k) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "$$\\begin{align}\n",
    "取a_k=\\frac{1}{k}，w_{k+1} = w_{k}-\\frac{1}{k}(w_k-x_k)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47dd552",
   "metadata": {},
   "source": [
    "### I.5 收敛性证明\n",
    "略[用Dvoretzky's convergence theorem可以证明RM theorem]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5e6828",
   "metadata": {},
   "source": [
    "## II. SGD是RM算法的特殊形式\n",
    "### II.1 Batch Gradient Descent算法\n",
    "1. 求解问题：$\\underset{w}{min} J(w)=E[f(w, X)] $\n",
    "2. BGD算法\n",
    "$$\n",
    "\\begin{align}\n",
    "w_{k+1} & = w_{k}-a_k\\nabla_w E[f(w, X)] \\\\\n",
    "& = w_{k}-a_kE[\\nabla_w f(w, X)] \\\\\n",
    "样本均值估期望：\\\\\n",
    "w_{k+1} &= w_{k}-a_k[\\frac{1}{n}\\sum_{i=1}^{\\infty }\\nabla_w f(w_k, x_i)]\n",
    "\\end{align}\n",
    "$$\n",
    "3. SGD\n",
    "$$w_{k+1} = w_{k}-a_k\\nabla_w f(w_k, x_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de2ebe",
   "metadata": {},
   "source": [
    "### II.2 SGD是一个特殊的RM算法\n",
    "1. 将SGD转化为RM算法形式 \\\n",
    "(1) 问题：$\\underset{w}{min} J(w)  = E[f(w, X)]$ \\\n",
    "(2) 解法：$\\nabla_w J(w)=E[\\nabla_wf(w, X)]=0$ \\\n",
    "(3) 转化为RM算法可解问题：$g(w) = E[\\nabla_wf(w, X)]$ \\\n",
    "(4) <font color=blue>****用RM算法得到和SGD相同的迭代形式**</font>：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde g(w,\\eta ) \n",
    "& = g(w)+\\eta \\\\\n",
    "& = E[\\nabla_wf(w, X)] + \\left [ \\nabla_wf(w, x)-E[\\nabla_wf(w, X)]\\right ]  \\\\\n",
    "& = \\nabla_wf(w, x)\\\\\n",
    "其中:\\\\\n",
    "\\eta & = \\nabla_wf(w, x)-E[\\nabla_wf(w, X)] \\\\\n",
    "得到迭代形式：\\\\\n",
    "w_{k+1} & = w_k -a_k*\\nabla_wf(w_k, x_k)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d229b2",
   "metadata": {},
   "source": [
    "2. SGD的收敛条件：<font color=green>**对应RM算法的3个条件**</font> \\\n",
    "(1) $0<c_1\\le \\nabla ^2 _wf(w,X)\\le c_2$。此时，f(w,X)相对w是凸函数 \\\n",
    "(2) $\\sum^{\\infty}_{k=1}a_k=\\infty,且\\sum^{\\infty}_{k=1}a_k^2<\\infty$ \\\n",
    "(3) $\\{x_k\\}_{k=1}^{\\infty}是iid的$。此时可以证明$E\\eta=0$，用$|\\nabla f(w_k,x)|<\\infty$可以证明$E\\eta^2<\\infty$ \\\n",
    "$$\n",
    "\\begin{align}\n",
    "E\\eta & = E\\left (  \\nabla_wf(w, x)-E[\\nabla_wf(w, X)]  \\right )\\\\\n",
    "& = E[\\nabla_wf(w, x)]-E[\\nabla_wf(w, X)]，\\because \\{x_i\\}是iid的 \\\\\n",
    "& = 0\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "满足这3个条件后，$w_k$almost surely converge to $\\nabla_wEf(w, X)=0$的解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d3242e",
   "metadata": {},
   "source": [
    "### II.3 SGD算法的收敛性质\n",
    "1. 定义一个用于衡量收敛结果的指标：relative error\n",
    "$$\\delta_k = \\frac{\\left | \\nabla_wf(w_k, x_k)-E[\\nabla_wf(w_k, X)] \\right | }{\\left | E[\\nabla_wf(w_k, X)] \\right | } $$\n",
    "**含义**：<font color=blue>分子是stochastic gradient$\\nabla_wf(w_k, x_k)$与true gradient$E[\\nabla_wf(w_k, X)]$的距离，整个式子表示stochastic gradient与true gradient的相对距离。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67ec643",
   "metadata": {},
   "source": [
    "2. relative error有性质：$$\n",
    "\\delta_k \\le \\frac{\\left | \\nabla_wf(w_k, x_k)-E[\\nabla_wf(w_k, X)] \\right | }{c\\left | w_k-w^* \\right | }\n",
    "$$\n",
    "<font color=grey>\n",
    "证明：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\because  E[\\nabla_wf(w^*, X)]& =0 \\\\\n",
    "\\therefore  E[\\nabla_wf(w_k, X)] & = E[\\nabla_wf(w_k, X)]-E[\\nabla_wf(w^*, X)] \\\\\n",
    " 根据中值定理：& \\exists \\tilde w_k \\in [w_k,w^*]，使得：\\\\\n",
    "\\left | E[\\nabla_wf(w_k, X)] \\right |  & = \\left |E[\\nabla_wf(w_k, X)]-E[\\nabla_wf(w^*, X)] \\right |\\\\\n",
    "& =\\left |(w_k-w^*)E[\\nabla^2_wf(\\tilde w_k, X)]\\right | \\\\\n",
    "& \\ge c\\left |w_k-w^*\\right |，\\because 当SGD满足收敛条件，就有\\nabla^2_wf(w, X) \\ge c_1 > 0  \\\\\n",
    "代入上面的定义即可得证\n",
    "\\end{align}\n",
    "$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c55b9f",
   "metadata": {},
   "source": [
    "3. **SGD的convergence pattern** \\\n",
    "上述不等式的分母代表了$w_k$与最优值$w_*$的距离，relative error与之成反比。**结论：<font color=green>当$w_k$距离最优值很远的时候，分母大，relative error很小，所以stochastic gradient与true gradient的差异就很小。这时候使用SGD与BGD的效果接近。反之，当$w_k$逼近真实值之后，分母变大，这时候relative error很可能较大，SGD在这时候与BGD的差异很大，此时convergence轨迹会出现更多randomness。</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6f605",
   "metadata": {},
   "source": [
    "## III. RM 算法的典型应用：求$E[f(x)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b211fc7f",
   "metadata": {},
   "source": [
    "1. 问题设定 \\\n",
    "原问题：求$Ef(X)=w$ \\\n",
    "转化为RM问题：$g(w)=w-Ef(X)=0$<font color=red>[注：不能是$g(w)=Ef(X)-w=0$]</font>\n",
    "2. 已知样本信息：$\\{(x_i, f(x_i))\\}_{i=1}^N$\n",
    "3. 构造迭代要素：\n",
    "$$\n",
    "\\begin{align} \n",
    "\\tilde g(w, \\eta) & = w-f(x) \\\\\n",
    "\\eta & = \\tilde g(w, \\eta) - g(w) \\\\\n",
    "& = [w-f(x)]-[w-Ef(X)] \\\\\n",
    "& = Ef(X)-f(x)\n",
    "\\end{align}\n",
    "$$\n",
    "4. 得到迭代式\n",
    "$$\n",
    "\\begin{align} \n",
    "w_{k+1} \n",
    "& = w_k - \\alpha_k\\tilde g(w_k, \\eta) \\\\\n",
    "& = w_k - \\alpha_k(w_k-f(x_k)) \\\\\n",
    "本质：\\\\\n",
    "新估计 & = 旧估计 - 学习速度*[旧估计 - 新样本值]\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
