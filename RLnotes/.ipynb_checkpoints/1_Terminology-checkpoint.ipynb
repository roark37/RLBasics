{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692eb9fe",
   "metadata": {},
   "source": [
    "# 基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238ca6f",
   "metadata": {},
   "source": [
    "## I. Terminologyies\n",
    "<img src=\"./pics/agentandenvironment.png\" style=\"zoom:60%\">\n",
    "\n",
    "1. Agent和Environment：\\\n",
    "Agent在Environment中执行action，并得到对应reward。Agent是执行行为的主体，它在环境中如果执行action是reinforcement learning学习的目标。比如达里奥游戏中的游戏任务就是一个Agent。\n",
    "2. state s： \\\n",
    "agent所处的环境在不同时期的状态，agent根据不同的状态选择执行action，并得到对应的rewards。比如：达里奥中的当前state就是游戏当前所处的那一帧画面\n",
    "3. action a：\\\n",
    "agent在环境中的行为动作称为action。比如：达里奥中，agent可以做出的行为包括{左，右，上}，因此，$a \\in \\{left, right, up\\}$。\n",
    "4. <font color=blue>policy function $\\pi$：$\\pi(a|s)=P_\\pi(A=a|S=s)=P_\\pi(a,s)$</font>\\\n",
    "agent在给定环境状态s下所采取的策略称为policy function，简称policy。policy有随机性，给定state s，agent所采取的action的分布就是policy。\n",
    "5. <font color=blue>state transition function：$P(S_{t+1}|S_t=s, A_t=a)$</font>\\\n",
    "$$old\\ state \\overset{action}{\\rightarrow} new\\ state$$\n",
    "状态转移函数描述了给定t时刻状态s，agent执行action a后，下一时刻状态$s_{t+1}$的分布情况。由于环境本身有随机性，因此状态转移函数也有随机性。比如，马里奥中，当马里奥前进吃掉一个coin之后，旁边的demon下一步可能靠近它，也可能走向反方向。\n",
    "6. <font color=blue>reward R：$r_t=R(s_t, a_t, s_{t+1})$</font>\\\n",
    "agent在给定的环境状态s中执行一次action就会得到对应的一个reward。$a_t$的reward的大小由$s_t$，$a_t$和$s_{t+1}$决定。比如：达里奥中，收集硬币R=1，什么都不做R=0，通关成功R=1000，通关失败R=-1000.\n",
    "7. utility/return U：\\\n",
    "从t时刻开始一直到行为结束的n时刻得到的所有rewards折现之和。$\\gamma是折现率$\n",
    "$$U_t=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+...++\\gamma^{n-t} R_{n}$$\n",
    "$$U_0=R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...++\\gamma^{n} R_{n}$$\n",
    "agent的目标是找到最合理的action使得预期总体回报最大化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bba9ba",
   "metadata": {},
   "source": [
    "## II. 随机性\n",
    "1. 整个状态变化过程中有两个随机性的来源：\\\n",
    "① randomness in <font color=red>**actions**</font> from policy function: $A\\sim \\pi(a|s)=P(a|s)$ \\\n",
    "② randomness in <font color=red>**states**</font> from state transition function: $S \\sim P(s_{t+1}|s_t, a_t)$\n",
    "2. rewards中的随机性：\\\n",
    "由于action有随机性，如果按照给定策略$\\pi$行动，那么实际发生的$a_t$有随机性。同时，$s_t$取决于$P(s_t|s_{t-1},a_{t-1})$也有随机性，因此$r_t=R(s_t, a_t)$的随机性同时受action和states中的随机性影响。\n",
    "3. utility中的随机性： \\\n",
    "rewards中的随机性会自然传递给utility。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31272da",
   "metadata": {},
   "source": [
    "## III. value functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0291ed",
   "metadata": {},
   "source": [
    "1. **utility function**\n",
    "\\begin{align} \n",
    "  U & = R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...++\\gamma^{n} R_{n} \\\\\n",
    "& =  {\\textstyle \\sum_{t=0}^{n}}\\gamma ^tR_t \\\\\n",
    "& =  {\\textstyle \\sum_{t=0}^{n}}\\gamma ^tR(s_t, a_t, s_{t+1}) \\\\\n",
    "& 其中，n是time\\ horizon\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42479cd4",
   "metadata": {},
   "source": [
    "2. **action-value function**：<font color=orange>**评估action的优劣**</font>\n",
    "$$\\begin{align} \n",
    " Q_{\\pi}(s, a) & = E[ U|\\pi, s_0=s, a_0=a ] \\\\\n",
    " & =E[ {\\textstyle \\sum_{t=0}^{n}}\\gamma ^tR(s_t, a_t, s_{t+1})|\\pi, s_0=s, a_0=a ] \\\\\n",
    " \\\\\n",
    " Q^*(s, a) & = \\max_{\\pi} Q_{\\pi}(s, a) \\\\\n",
    " & = \\max_{\\pi}  E[ U|\\pi, s_0=s, a_0=a ] \\\\\n",
    "& =\\max_{\\pi} E[ {\\textstyle \\sum_{t=0}^{n}}\\gamma ^tR(s_t, a_t, s_{t+1})|\\pi, s_0=s, a_0=a ]\n",
    "\\end{align}$$\n",
    "**理解**：\\\n",
    "① <font color=red>$Q_{\\pi}(s, a)$给定当前state s，假定从下一时刻(t=1)起都使用策略$\\pi$的条件下，action a可以获得的平均总回报。</font><font color=blue>衡量给定策略$\\pi$，在当前状态s下选择action a怎么样</font> \\\n",
    "② <font color=red>$Q^*(s, a)$给定当前state s，假定从下一时刻(t=1)起都使用最优策略的条件下，action a可以获得的平均总回报。</font><font color=blue>衡量在最优决策条件下，在当前状态s下选择action a怎么样</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c2b15",
   "metadata": {},
   "source": [
    "3. **state-value function**：<font color=orange>**评估state的优劣**</font>\n",
    "$$\\begin{align} \n",
    "V_{\\pi}(s) & = E_A[ Q_{\\pi}(s, A) ] \\ ,\\ 其中A\\sim \\pi(a|s)\\\\\n",
    "& = E[ {\\textstyle \\sum_{t=0}^{n}}\\gamma ^tR(s_t, a_t, s_{t+1})|\\pi, s_0=s ] \\\\\n",
    "\\\\\n",
    "V^*(s)& = \\max_{\\pi} V_{\\pi}(s)\\\\\n",
    "& = \\max_{\\pi} E[ U|\\pi, s_0=s ] \\\\\n",
    "& = \\max_{\\pi} E[ {\\textstyle \\sum_{t=0}^{n}}\\gamma ^tR(s_t, a_t, s_{t+1})|\\pi, s_0=s ]\n",
    "\\end{align}$$\n",
    "**理解**：\\\n",
    "① <font color=red>$V_{\\pi}(s)$给定策略 𝜋的条件下，当前state s可以获得的平均总回报。</font><font color=blue>衡量给定策略$\\pi$，agent当前所处状态s怎么样</font> \\\n",
    "② <font color=red>$V^*(s)$给定当前state s，假定一直使用最优策略的条件下，可以获得的平均总回报。</font><font color=blue>衡量在最优决策条件下，当前所处状态s怎么样</font> \\\n",
    "③ $E_s(V_{\\pi}(s))$可以衡量策略$\\pi$本身的优劣"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa14b425",
   "metadata": {},
   "source": [
    "4. state-value func和action-value func的关系\n",
    "$$V^*(s) = \\max_{a} Q^*(s, a)\\ , \\ \\ \\pi^*(s) = \\underset{a}{argmax} Q^*(s, a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc2ce1",
   "metadata": {},
   "source": [
    "## IV. 公式小结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7795b71f",
   "metadata": {},
   "source": [
    "#### IV.1 定义MDP\n",
    "1. 状态 \\\n",
    "(1)状态集$\\mathcal{S}$，如果状态是离散且有限的，则$\\mathcal{S}=\\{s^{(1)},s^{(2)},...,s^{(K)} \\}$。 \\\n",
    "(2)一个起始状态$s_0$\\\n",
    "(3)一个或多个终止状态\n",
    "2. 动作：$a\\in \\mathcal{A} $\n",
    "3. 策略：$\\pi$：$\\pi(a|s)=P_{\\pi}(a|s)=P_{\\pi}(A=a|S=s)$\n",
    "4. 回报 \\\n",
    "(1)回报函数(reward function)：$R_t=R(s_t, a_t, s_{t+1})$ \\\n",
    "(2)回报的折扣因子：$\\gamma$\n",
    "5. 状态转移函数(transition function)：$T(历史状态, a, 下一状态)$。马尔科夫条件下，状态转移函数可以表示为：$T(s_t, a_t, s_{t+1}) =P(s_{t+1}|s_t, a_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1d7fd",
   "metadata": {},
   "source": [
    "#### IV.2 Reward function和Utility\n",
    "1. 已知(a, s)：此时只考虑下一状态的分布\n",
    "$$\n",
    "\\begin{align}\n",
    "E(R|a, s) & = E_{T}(R|s, a)   \\\\\n",
    "& = E_{P(s^{'}|s, a)}(R|s, a)   \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}} P(s^{'}|s, a)R(s, a, s^{'})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73438752",
   "metadata": {},
   "source": [
    "2. 已知(s)：此时同时考虑action的分布和下一状态的分布 \n",
    "$$\n",
    "\\begin{align}\n",
    "E(R|s) & = E_{a\\sim A}[E(R|s, a)]   \\\\\n",
    "& = E_{a\\sim A}[\\sum_{s^{'}\\in \\mathcal{S}} P(s^{'}|s, a)R(s, a, s^{'})] \\\\\n",
    "& = \\sum _{a\\in \\mathcal{A}}\\pi (a|s)\\sum_{s^{'}\\in \\mathcal{S}} P(s^{'}|s, a)R(s, a, s^{'})\n",
    "\\end{align} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac91a1",
   "metadata": {},
   "source": [
    "3. 效用的迭代形式 \\\n",
    "(1)time horizon有限的条件下\n",
    "$$\n",
    "\\begin{align}\n",
    "U_t\n",
    "& =R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+...+\\gamma^{n} R_{t+n} \\\\\n",
    "& =R_t+\\gamma \\left [ R_{t+1}+\\gamma R_{t+2}+...+ \\gamma^{n-1} R_{t+n} \\right ] \\\\\n",
    "& =R_t+\\gamma U_{t+1}\n",
    "\\end{align} \n",
    "$$\n",
    "(2)time horizon无限的条件下\n",
    "$$\n",
    "\\begin{align}\n",
    "U_t\n",
    "& =R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+...+\\gamma^{n} R_{t+n}+... \\\\\n",
    "& =R_t+\\gamma \\left [ R_{t+1}+\\gamma R_{t+2}+...+ \\gamma^{n-1} R_{t+n}+...  \\right ] \\\\\n",
    "& =R_t+\\gamma U_{t+1}\n",
    "\\end{align} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67ef54",
   "metadata": {},
   "source": [
    "#### IV.3 贝尔曼期望方程\n",
    "<font color=red>**本质：time horizon无限的条件下，求Utility的期望值**</font>\n",
    "1. <font color=orange>**性质：time horizon无限的条件下，期望效用与time无关。**</font>\n",
    "$$\n",
    "\\color{orange}  {E(U_{t=i}|s) =E(U_{t=j}|s)=E(U|s^{'}) , 其中i\\ne j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27a45fa",
   "metadata": {},
   "source": [
    "2. <font color=blue>**time horizon无限的条件下，求状态价值函数：** </font>\n",
    "$$\n",
    "\\color{Blue}  {V^{\\pi}(s)  = E(U|s) = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma V^{\\pi}(s) \\right ]} \n",
    "$$\n",
    "证明：\\\n",
    "(1)已知s，求效用的期望\n",
    "$$\n",
    "\\begin{align}\n",
    "E(U_0|s)\n",
    "& = E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s) \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s,a)\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s,a,s^{'})\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E\\left ( R_1+\\gamma R_{2}+...+\\gamma^{n-1} R_{n}+...|s, a, s^{'} \\right )  \\right ] \\\\ \n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U_1|S_0=s, A_0=a, S_1=s^{'}) \\right ] \\\\\n",
    "& \\because 马尔科夫条件下，U_1不受S_0和A_0取值的影响 \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U_1|s^{'}) \\right ]\n",
    "\\end{align} \n",
    "$$\n",
    "**(2)time horizon无限的条件下，状态价值函数的递归形式**\n",
    "$$\n",
    "\\begin{align}\n",
    "\\because E(U_t|s) & =E(U|s), E(U_{t+1}|s^{'})=E(U|s^{'})\\\\\n",
    "\\therefore E(U|s) & = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U|s^{'}) \\right ]\\\\\n",
    "\\therefore V^{\\pi}(s) & = E(U|s) = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma V^{\\pi}(s) \\right ]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c31e68",
   "metadata": {},
   "source": [
    "3. <font color=blue>**time horizon无限的条件下，求行为价值函数：** </font>\n",
    "$$\n",
    "\\color{Blue} {\n",
    "\\begin{align}\n",
    "Q^{\\pi}(s,a)  = E(U|s,a) & = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma V^{\\pi}(s^{'}) \\right ] \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma \\sum_{a^{'}\\in \\mathcal{A}}\\pi(a^{'}|s^{'})Q^{\\pi}(s^{'},a^{'}) \\right ] \n",
    "\\end{align}\n",
    "}\n",
    "$$\n",
    "证明：\\\n",
    "(1)已知(s,a),求效用的期望\n",
    "$$\n",
    "\\begin{align}\n",
    "E(U_0|s,a)\n",
    "& = E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s,a) \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s,a,s^{'})\\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E\\left ( R_1+\\gamma R_{2}+...+\\gamma^{n-1} R_{n}+...|s, a, s^{'} \\right )  \\right ] \\\\ \n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U_1|s, a, s^{'}) \\right ] \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U_1|s^{'}) \\right ]\n",
    "\\end{align}\n",
    "$$\n",
    "**(2)time horizon无限的条件下，行为价值函数的递归形式**\n",
    "$$\n",
    "\\begin{align}\n",
    "\\because E(U_t|s,a) & = E(U|s,a), E(U_{t+1}|s^{'})=E(U|s^{'}) \\\\\n",
    "\\therefore E(U|s,a) & = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U|s^{'}) \\right ] \\\\\n",
    "\\therefore Q^{\\pi}(s,a) & =E(U|s,a) =\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma V^{\\pi}(s^{'}) \\right ]\n",
    "\\end{align}\n",
    "$$\n",
    "(3)转换成贝尔曼期望方程的形式：\n",
    "$$\n",
    "\\begin{align}\n",
    "Q^{\\pi}(s,a) & = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U|s^{'}) \\right ] \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma \\sum_{a^{'}\\in \\mathcal{A}}\\pi(a^{'}|s^{'})E(U|s^{'},a^{'}) \\right ] \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma \\sum_{a^{'}\\in \\mathcal{A}}\\pi(a^{'}|s^{'})Q^{\\pi}(s^{'},a^{'}) \\right ] \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
