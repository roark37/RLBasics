{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4540b0a",
   "metadata": {},
   "source": [
    "# MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fdfe33",
   "metadata": {},
   "source": [
    "## I. MDP（markov decision process）\n",
    "### I.1 定义MDP\n",
    "1. **Markov property：**\\\n",
    "① 指给定当前状态，未来和过去的状态是独立的。\\\n",
    "② 在MDP中，'Markov'是指action outcome只取决于当前state，与前序states无关。$$\\begin{align}\n",
    "P(S_{t+1} = s^{'}|S_t=s_t,A_t=a_t, S_{t-1},A_{t-1},..., S_0) = P(S_{t+1} = s^{'}|S_t=s_t,A_t=a_t)\\\\\n",
    "\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "2. **MDP的定义：在完全可观察(fully observable)的、随机的环境中，transition model符合markovian的条件下，用可加的rewards做序列决策的过程。** \\\n",
    "注意这个定义中的要素：\\\n",
    "① <font color=green>fully observable environment是指transition function和reward function已知</font> \\\n",
    "② random environment: transition function有随机性 \\\n",
    "③ markovian transition function: $T(s, a, s^{'})=P(s^{'}|s, a)$ \\\n",
    "④ <font color=red>additive rewards: utility可以用rewards的其他函数形式来定义。但是如果utility要满足preference-independent，那么用sum of discounted rewards是唯一符合该假设的函数形式。</font> \\\n",
    "<font color=blue>[详见AI：a modern approach 4th ch17.1] </font>\n",
    "\n",
    "3. **用MDP分析问题所需的要素：**\\\n",
    "① 一系列状态S，$s\\in S$ \\\n",
    "② 一系列行动A，$a\\in A$ \\\n",
    "③ 一个起始状态 \\\n",
    "④ 一个或者多个终止状态 \\\n",
    "⑤ rewards折扣因子$\\gamma$ \\\n",
    "⑥ 一个transition function $T(s, a, s^{'})=P(s^{'}|s, a)$ \\\n",
    "⑦ 一个reward function $R(s, a, s^{'})$ \\\n",
    "<font color=orange>**图示: movements of an agent through a MDP:**</font> \\\n",
    "$$\\begin{align} \n",
    "& s_0\\overset{a_0}{\\rightarrow} s_1\\overset{a_1}{\\rightarrow} s_2\\overset{a_2}{\\rightarrow} s_3\\overset{a_3}{\\rightarrow}...\\\\\n",
    "& a_t\\sim P(a_t|s_t)=\\pi(s_t)\\\\ \n",
    "& s_t\\sim P(s_t|s_{t-1}, a_{t-1})=T(s_{t-1}, a_{t-1}, s_t) \\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "4. 求解MDP问题的目标：求policy $\\pi(s)$ \\\n",
    "① 策略的质量由该策略导致的一系列环境状态s对应的回报r折现得到的效用的期望值$E(U|\\pi)$决定。（expected utility of the possible environment histories generated by that policy）$$\\begin{align} \n",
    "E(U|\\pi) & = E( {\\textstyle \\sum_{t=0}^{n}} R_t|\\pi) \\\\\n",
    "& = E( {\\textstyle \\sum_{t=0}^{n}} R(s_t, a_t, s_{t+1})|\\pi) \\\\\n",
    "& = E(R_0+\\gamma R_1+\\gamma ^2R_2+...+\\gamma ^nR_n|\\pi ) \\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "② 最优策略：\n",
    "$$\\begin{align} \n",
    "\\pi ^*=\\underset{\\pi}{argmax}\\ E(U|\\pi) \\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "5. **MDP问题的解法：dynamic programming** \\\n",
    "simplifying a problem by recursively breaking it into smaller pieces and remembering the optimal solutions to the pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad99257",
   "metadata": {},
   "source": [
    "### I.2 决策时限和策略的稳定性\n",
    "MDP问题考察的时间长度可能是finite horizon或者infinite horizon的，两种情况下最优决策会有所不同。 \\\n",
    "<font color=green>finite horizon的意思是说，有一个固定的时间长度n，在这个时间之后的rewards不影响utility。比如，下围棋约定只能各走20子，20子之后，就算胜负扭转也不计入结果。</font>\n",
    "1. 在<font color=orange>**infinite horizon**</font>条件下做决策: 最优决策是<font color=blue>**稳定的(optimal policy is stationary)**</font> \\\n",
    "· 含义：<font color=purple>如果决策时间长度是infinite horizon的，那么最优策略只取决于状态s。</font> \\\n",
    "· 理解：<font color=red>**[rk's note]**</font> \\\n",
    "① 时间对utility的影响这时候是通过discount factor $\\gamma$来作用的，因此不同在函数中单独考虑time horizon。 \\\n",
    "② <font color=blue>由于reward可加，且$R_t=R(s_t, a_t, s_{t+1})$即当期reward由$s_t, a_t$和transition function决定。而transition model符合markovian，也就是$s_{t+1}$只取决于$s_t和a_t$。因此，给定transition function的条件下，reward $R_t$由$s_t和a_t$决定。这意味着: \\\n",
    "i. policy只看当期s，不用考虑前序状态，因此$\\pi=\\pi(s_t)=P(a|s_t)$。 \\\n",
    "ii. 给定状态$s$，最优policy $\\pi^*(s)$是稳定分布，也就是只要s不变，policy的分布也不变。  </font> \n",
    "$$\n",
    "$$\n",
    "2. 在<font color=orange>**finite horizon**</font>条件下做决策: 最优决策是<font color=blue>**不稳定的(optimal policy is nonstationary)**</font>  \\\n",
    "· 含义：<font color=purple>如果决策时间长度是finite horizon的，那么最优策略不仅取决于状态s，还取决于剩余的time horizon。</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb542bc",
   "metadata": {},
   "source": [
    "## II. Bellman equation\n",
    "1. 概念符号：\\\n",
    "① 给定策略$\\pi$，start state为s，start action为a，infinite horizon条件下的期望效用：\n",
    "$$\\begin{align} \n",
    "  Q^{\\pi}(s, a) =E(U|\\pi,s_0=s, a_0=a)  = E( {\\textstyle \\sum_{t=0}^{\\infty }} \\gamma ^tR_t|\\pi,s_0=s, a_0=a)\\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "② 给定策略$\\pi$，start state为s，infinite horizon条件下的期望效用：\n",
    "$$\\begin{align} \n",
    "V^{\\pi}(s)=E(U|\\pi,s_0=s)  = E_AQ^{\\pi}(s, a)\\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "③ start state为s，start action为a，infinite horizon条件下，从下一步开始用最优策略达到的期望效用：\n",
    "$$\\begin{align} \n",
    "Q^{*}(s, a)=\\underset{\\pi}{max}\\ Q^{\\pi}(s, a)=\\underset{\\pi}{max}\\ E(U|\\pi, s_0=s, a_0=a)\\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "④ start state为s，infinite horizon条件下，用最优策略达到的期望效用：\n",
    "$$\\begin{align} \n",
    "V^*(s)=V^{\\pi^*}(s)=\\underset{\\pi}{max}\\ V^{\\pi}(s)=\\underset{\\pi}{max}\\ E(U|\\pi,s_0=s)\\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "⑤ infinite horizon条件下，用最优策略：\n",
    "$$\\begin{align} \n",
    "\\pi^*_s=\\underset{\\pi}{argmax}\\ V^{\\pi}(s)\\\\\n",
    "\\end{align}$$\n",
    "注1：<font color=green>虽然U的结果与起点时刻的state有关，但是当使用discounted utility，且infinite horizon时，$\\pi^*$与起点时刻的state无关。$$\\pi^*=\\pi^*_s,\\ \\forall s\\in S$$</font> \n",
    "注2：<font color=green>$\\pi^*=\\pi^*_s$表示start state为s时的整体策略，$\\pi^*(s)$表示当前state是s，当前步的最优策略。\n",
    "$$\\begin{align} \n",
    "\\pi^*(s)=\\underset{a}{argmax}Q^{*}(s, a)\\\\\n",
    "\\end{align}$$</font> \\\n",
    "⑥ 相互关系 \\\n",
    "<font color=blue>$$\\begin{matrix}\n",
    " Q^{\\pi}(s, a) & \\overset{\\pi=\\pi^*}{\\rightarrow} & Q^{*}(s, a)\\\\\n",
    "|& & |\\\\\n",
    " {\\scriptsize E_{A\\sim \\pi}Q^{\\pi}(s, a)}   &  & {\\scriptsize E_{A\\sim \\pi^{*}}Q^{*}(s, a)}\\\\\n",
    "|& & |\\\\\n",
    " V^{\\pi}(s) & \\overset{\\pi=\\pi^*}{\\rightarrow} & V^{*}(s)\n",
    "\\end{matrix}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba1ac47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T06:41:09.722137Z",
     "start_time": "2023-12-03T06:41:09.703762Z"
    }
   },
   "source": [
    "2. recursive definition of utility/value \\\n",
    "① <font color=blue>$U_t=R_t+\\gamma U_{t+1}$</font> \\\n",
    "<font color=gray>证明：$$\\begin{align} \n",
    "U_t & = R_{t} + \\gamma R_{t+1} + \\gamma ^2 R_{t+2} + ... +\\gamma ^n R_{t+n} + ...\\\\\n",
    "& = R_t + \\gamma (R_{t+1} + \\gamma ^1 R_{t+2} + ... +\\gamma ^{n-1} R_{t+n} + ...)\\\\\n",
    "& = R_t+\\gamma U_{t+1}\n",
    "\\end{align}$$ </font>\n",
    "② <font color=blue>$Q^{\\pi }(s,a) = \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{\\pi }(s^{'})] $</font>  \\\n",
    "<font color=gray>证明：$$\\begin{align} \n",
    "Q^{\\pi }(s,a) & = E[R_{0} + \\gamma R_{1} + \\gamma ^2 R_{2} + ... +\\gamma ^n R_{n} + ...|\\pi, s_0=s,a_0=a]\\\\\n",
    "& = E[R(s,a,s_{1}) + \\gamma R(s_1,a_1,s_{2}) + ... +\\gamma ^n R(s_n,a_n,s_{n+1}) + ...|\\pi]\\\\\n",
    "& = \\sum_{s^{'}}^{}P(s^{'}|s,a) [R(s,a,s^{'})+ \\gamma E[R(s^{'},a_1,s_{2}) + ... +\\gamma ^{n-1} R(s_n,a_n,s_{n+1}) + ...|\\pi]] \\\\\n",
    "& = \\sum_{s^{'}}^{}P(s^{'}|s,a)[R(s,a,s^{'})+\\gamma V^{\\pi }(s^{'})] \\\\\n",
    "& = \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{\\pi }(s^{'})] \n",
    "\\end{align}$$ </font>\n",
    "③ <font color=blue>$Q^{*}(s,a) = \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{*}(s^{'})]$</font> \\\n",
    "<font color=gray>证明：代入$\\pi=\\pi^*$即可\n",
    "$$\\begin{align} Q^{*}(s,a) = \\underset{\\pi }{max}\\ Q^{\\pi }(s,a)= \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{*}(s^{'})]\n",
    "\\end{align}$$ </font>\n",
    "④ <font color=blue>$V^*(s)=\\underset{a}{max} Q^*(s, a)=\\underset{a}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{*}(s^{'})]$ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d673c",
   "metadata": {},
   "source": [
    "3. **Bellman equation** \\\n",
    "<font color=blue>$$V^*(s)=\\underset{a}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{*}(s^{'})]$$</font>\n",
    "含义：如果agent选择最优策略，那么当前状态的效用等于当期回报的加权平均和下期效用折现值的加权平均之和。他们使用的加权权重都是由transition function决定的下期状态$s^{'}$发生概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f618f5",
   "metadata": {},
   "source": [
    "## III. 求解Bellman equation的4种方法\n",
    "### III.1 value iteration\n",
    "1. 算法描述 \\\n",
    "已知transition model $T(s,a,s^{'})$，rewards $R(s,a,s^{'})$，收敛精度$\\epsilon$，discount $\\gamma$ \\\n",
    "初始化$V_0(s)=0$ \\\n",
    "迭代: \\\n",
    "$$\\begin{align} \n",
    "repeat:&&\\\\\n",
    "&\\delta=0 \\ &\\\\\n",
    "&for\\ s\\ in\\ S:&\\\\\n",
    "& & V_{k+1}(s) \\leftarrow \\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V_k(s^{'})]\\\\\n",
    "& & if\\ \\delta < |V_{k+1}(s)-V_{k}(s)|,\\ then \\ \\delta = |V_{k+1}(s)-V_{k}(s)|\\\\\n",
    "until:&\\delta<\\epsilon\\frac{(1-\\gamma )}{\\gamma}&\n",
    "\\end{align}$$\n",
    "2. 算法复杂度：$O(|S|^2|A|)$ \\\n",
    "分析：要对所有$|S|^2$组pair of states做迭代。每次迭代要计算|A|个action在一组pair of states下的value。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8676599",
   "metadata": {},
   "source": [
    "3. 收敛性：value iteration会收敛到唯一最优解 \\\n",
    "证明："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba59e469",
   "metadata": {},
   "source": [
    "### III.2 policy iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a47ed0",
   "metadata": {},
   "source": [
    "### III.3 linear programming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04273bb8",
   "metadata": {},
   "source": [
    "### III.4 online algorithms for MDPs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80021ba1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
