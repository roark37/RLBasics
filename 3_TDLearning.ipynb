{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2558313c",
   "metadata": {},
   "source": [
    "# TD Learning\n",
    "## I. reinforcement learning的基本概念\n",
    "1. online planning和offline planning \\\n",
    "① **offline planning** \\\n",
    "知道transition function和rewards function，可以直接求最优策略。MDP就是典型的offline planning。\\\n",
    "② **online planning** \\\n",
    "不知道transition function和rewards function，无法直接求最优策略。agent需要通过exploration来获取环境信息。\n",
    "\n",
    "2. sample and episode \\\n",
    "① **sample**: 在online planning中，agent的一次行为反馈过程$(s, a, s^{'}, r)$称为一个sample。 \\\n",
    "② **episode**: 在online planning中，agent持续take action，并collect samples，直到达到terminal state的整个过程称为一个episode。通常要经历多个episods才能完成exploration。\n",
    "\n",
    "3. model-based learning and model-free learning \\\n",
    "① <font color=blue>**model-based learning**</font>：先利用exploration得到的episodes数据，来构造model估计transition function和rewards function，然后就可以将问题转化为MDP问题求解。\\\n",
    "② <font color=blue>**model-free learning**</font>：不估计transition function和rewards function，而是直接估计value或者Q-values of states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ca9dc",
   "metadata": {},
   "source": [
    "## II. model-based learning\n",
    "1. 估计方法 \\\n",
    "① 估计transition function：$\\hat T(s, a, s^{'})=\\frac{\\#(s, a, s^{'})}{\\#(s, a)}$ \\\n",
    "② 估计reward function：$\\hat R(s, a, s^{'})= R(s, a, s^{'})$\n",
    "2. 收敛条件：根据大数定律，增加样本量可以让估计趋于真实值。\n",
    "3. 应用场景：保留所有可能的$(s, a, s^{'})$table需要的存储成本很大，所以model-based learning多用在存储能力的足够的场景下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ada77",
   "metadata": {},
   "source": [
    "## III. model-free learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec67dc4",
   "metadata": {},
   "source": [
    "### III.1 passive reinforcement learning\n",
    "在passive reinforcement learning中，会先给agent一个policy，然后agent用这个policy来经历episodes，以learn环境中的状态信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80aab32",
   "metadata": {},
   "source": [
    "#### III.1.1 direct learning\n",
    "1. 思路：根据给定策略条件下的value function定义：$$\\begin{align} \n",
    "V^{\\pi}(s)& =E(U|\\pi,s_0=s)\\\\\n",
    "& =E\\left [ \\sum_{t=0}^{\\infty } \\gamma ^tR(s_t, \\pi(s_t), s_{t+1}) |s_0=s\\right ] \\\\\n",
    "\\end{align}$$可以直接用抽样方法找不同states的$V^{\\pi}(s)$估计值。\n",
    "2. 方法：固定策略$\\pi$，直接采样$V^{\\pi}(s)$。1个以$s_i$为起点的episode得到1个$V^{\\pi}(s_i)$的值。用均值作为$V^{\\pi}(s_i)$的估计值。\n",
    "$$\\begin{align} \n",
    "& sample_1 = V^{\\pi}(s)=R(s,\\pi(s),s^{'}_1)+\\sum_{t=1}^{terminal } \\gamma ^tR(s_{1,t}, \\pi(s_{1,t}), s_{1,t+1})\\\\\n",
    "& sample_2 = V^{\\pi}(s)=R(s,\\pi(s),s^{'}_2)+\\sum_{t=1}^{terminal } \\gamma ^tR(s_{2,t}, \\pi(s_{2,t}), s_{2,t+1})\\\\\n",
    "& ...\\\\\n",
    "& sample_n = V^{\\pi}(s)=R(s,\\pi(s),s^{'}_n)+\\sum_{t=1}^{terminal } \\gamma ^tR(s_{n,t}, \\pi(s_{n,t}), s_{n,t+1})\\\\\n",
    "& V^{\\pi }(s) \\leftarrow \\frac{1}{n}\\sum_{i}sample_i  \n",
    "\\end{align}$$\n",
    "3. 问题：信息浪费，导致准确估计Value function所需要的episode量很大。因为1个以$s_i$为起点的episode只采纳对应的total rewards作为有效信息。这个episode在exploration过程中rewards value包含了不同的states之间transition的关联信息，都没有被用到。这也导致准确估计Value function所需要的episode量很大。\n",
    "4. **应用要点**：<font color=green>direct learning（包括后面的temporal difference learning）只是找$V^{\\pi}$的方法。但原本的目标是找到最优策略。所以找到$V^{\\pi}$之后，通常需要再结合model-based learning方法来获得transition function和rewards function的信息。\\\n",
    "有了$V^{\\pi}(s),T(s, \\pi(s), s^{'}),R(s, \\pi(s), s^{'})$的信息后，再用$$Q^*(s, a)=\\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{*}(s^{'})]$$来迭代最优策略。\\\n",
    "所以direct learning和TD learning相当于完成了用policy iteration求解MDP问题时的policy evaluation的那一步，而policy improvement/extraction的那一步还需要额外的model-based learning的方法来做。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c453fa",
   "metadata": {},
   "source": [
    "#### III.1.2 基于direct learning做优化，利用中间信息\n",
    "1. 思路：direct learning方法只用了total rewards value，没有使用中间状态的reward信息，因此造成了信息浪费，一种解决这个问题的思路是，利用中间信息来做policy evaluation。<font color=red>不再用episode为抽样的单位，而是用一次action-reward为抽样的单位。</font>\\\n",
    "· <font color=blue>**policy evaluation**</font>：给定策略$\\pi$，可以用迭代法求解value/utility:$$\\begin{align} \n",
    "& V^{\\pi}_0(s)=0\\\\\n",
    "& iterate:V^{\\pi}_{k+1}(s)\\leftarrow \\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,\\pi(s),s^{'})+\\gamma V^{\\pi}_{k}(s^{'})]\\\\\n",
    "& until:V^{\\pi}(s) \\ stable\n",
    "\\end{align}$$\n",
    "在没有T和R信息的情况下，用抽样的方法求解。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf39d89",
   "metadata": {},
   "source": [
    "2. 方法： 从$V^{\\pi}_0(s)=0$开始按下述方式抽样，直到收敛。\n",
    "$$\\begin{align} \n",
    "第1轮：\n",
    "& for\\ s\\ in\\ S:\\\\\n",
    "&sample_1 = R(s, \\pi(s), s^{'}_1)+\\gamma V^{\\pi}_0(s^{'}_1) \\\\\n",
    "&sample_2 = R(s, \\pi(s), s^{'}_2)+\\gamma V^{\\pi}_0(s^{'}_2) \\\\\n",
    "&... \\\\\n",
    "&sample_n = R(s, \\pi(s), s^{'}_n)+\\gamma V^{\\pi}_0(s^{'}_n) \\\\\n",
    "&V^{\\pi }_{1}(s) \\leftarrow \\frac{1}{n}\\sum_{i}sample_i \\\\\n",
    "\\\\\n",
    "第2轮：& for\\ s\\ in\\ S:\\\\\n",
    "&sample_1 = R(s, \\pi(s), s^{'}_1)+\\gamma V^{\\pi}_1(s^{'}_1) \\\\\n",
    "&sample_2 = R(s, \\pi(s), s^{'}_2)+\\gamma V^{\\pi}_1(s^{'}_2) \\\\\n",
    "&... \\\\\n",
    "&sample_n = R(s, \\pi(s), s^{'}_n)+\\gamma V^{\\pi}_1(s^{'}_n) \\\\\n",
    "&V^{\\pi }_{2}(s) \\leftarrow \\frac{1}{n}\\sum_{i}sample_i\\\\\n",
    "...\\\\\n",
    "第k轮：& for\\ s\\ in\\ S:\\\\\n",
    "& sample_1 = R(s, \\pi(s), s^{'}_1)+\\gamma V^{\\pi}_k(s^{'}_1) \\\\\n",
    "& sample_2 = R(s, \\pi(s), s^{'}_2)+\\gamma V^{\\pi}_k(s^{'}_2)\\\\\n",
    "& ...\\\\\n",
    "& sample_n = R(s, \\pi(s), s^{'}_n)+\\gamma V^{\\pi}_k(s^{'}_n) \\\\\n",
    "& V^{\\pi }_{k+1}(s) \\leftarrow \\frac{1}{n}\\sum_{i}sample_i  \\\\\n",
    "直到收敛\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646e55fa",
   "metadata": {},
   "source": [
    "3. 优点：每个transition的信息都被利用起来，没有信息浪费。比direct learning效率更高，因为可以learn from every single experience。\n",
    "4. 问题：为了求$V^{\\pi}(s)$，需要每次都从指定的time horizon，指定的状态s开始抽样，但实际环境中很可能难以精确控制这些因素。比如，状态难以复现的情况很常见。\n",
    "5. <font color=green>**[rk's note]改进的思路**：上面的抽样方式限制很大。实际上agent在环境中做action的时候，最方便的是按照既定策略$\\pi$和start state做完整个episode。如果能将这样的episode中每个action-reward的信息作为样本使用，那实现起来会方便很多。</font><font color=red>TD learning就是这样的思路。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8301c36",
   "metadata": {},
   "source": [
    "#### III.1.3 temporal difference learning (TD learning)\n",
    "1. **思路**：\\\n",
    "① 和前面基于direct learning改进的方法很相似，也是在不知道Transition和Reward函数的条件下，将每一步中间action-reward作为一个样本，用抽样的方法来估计policy evaluation的结果。\n",
    "$$V^{\\pi}_{k+1}(s)\\leftarrow \\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,\\pi(s),s^{'})+\\gamma V^{\\pi}_{k}(s^{'})]$$\n",
    "② 但是改变了如何使用抽样sample来计算$V^{\\pi}(s)$的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9b619",
   "metadata": {},
   "source": [
    "2. 方法：\n",
    "$$\\begin{align} \n",
    "& V^{\\pi }(s)_0=0 \\\\\n",
    "iterate: \\\\\n",
    "& nth\\ sample\\ of\\  V^{\\pi}(s): \\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ sample_n = R(s,\\pi(s),s^{'})+\\gamma V^{\\pi }(s^{'})\\\\\n",
    "& V^{\\pi }(s)_{n} \\leftarrow (1-\\alpha)*V^{\\pi }(s)_{n-1}+\\alpha* sample_n\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27dd49",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ae08c1c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb924358",
   "metadata": {},
   "source": [
    "### III.2 active reinforcement learning\n",
    "在active reinforcement learning中，没有固定的policy，agent会随时根据learn到的环境信息来更新policy。也就是exploration和exploiting同时进行。\n",
    "#### III.2.1 Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec1250",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bc24796",
   "metadata": {},
   "source": [
    "### III.4 Approximate Q-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
