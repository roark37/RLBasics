{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511e8689",
   "metadata": {},
   "source": [
    "# TD Learning\n",
    "## I. reinforcement learning的基本概念\n",
    "1. online planning和offline planning \\\n",
    "① **offline planning** \\\n",
    "知道transition function和rewards function，可以直接求最优策略。MDP就是典型的offline planning。\\\n",
    "② **online planning** \\\n",
    "不知道transition function和rewards function，无法直接求最优策略。agent需要通过exploration来获取环境信息。\n",
    "\n",
    "2. sample and episode \\\n",
    "① **sample**: 在online planning中，agent的一次行为反馈过程$(s, a, s^{'}, r)$称为一个sample。 \\\n",
    "② **episode**: 在online planning中，agent持续take action，并collect samples，直到达到terminal state的整个过程称为一个episode。通常要经历多个episods才能完成exploration。\n",
    "\n",
    "3. model-based learning and model-free learning \\\n",
    "① **model-based learning**：先利用exploration得到的episodes数据，来构造model估计transition function和rewards function，然后再用估计出来的function直接求解MDP。\\\n",
    "② **model-free learning**：不估计transition function和rewards function，而是直接估计value或者Q-values of states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ab496",
   "metadata": {},
   "source": [
    "## II. model-based learning\n",
    "1. 估计方法\n",
    "① 估计transition function：$\\hat T(s, a, s^{'})=\\frac{\\#(s, a, s^{'})}{\\#(s, a)}$\n",
    "② 估计reward function：$\\hat R(s, a, s^{'})= R(s, a, s^{'})$\n",
    "\n",
    "2. 收敛条件：根据大数定律，增加样本量可以让估计趋于真实值。\n",
    "\n",
    "3. 问题：保留所有可能的$(s, a, s^{'})$table需要的存储成本很大，所以model-based learning的使用范围有限。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8850fd",
   "metadata": {},
   "source": [
    "## III. model-free learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610020f6",
   "metadata": {},
   "source": [
    "### III.1 direct learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999a642",
   "metadata": {},
   "source": [
    "### III.2 temporal difference learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6989c644",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a9652b6",
   "metadata": {},
   "source": [
    "### III.3 Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83096a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c3a0ae5",
   "metadata": {},
   "source": [
    "### III.4 Approximate Q-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
