{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28312109",
   "metadata": {},
   "source": [
    "# MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159c410",
   "metadata": {},
   "source": [
    "## I. MDP\n",
    "### I.1 定义MDP\n",
    "1. ‘Markov’的含义：\\\n",
    "① 一般场景下，'Markov'是指给定当前状态，未来和过去的状态是独立的。\\\n",
    "② 在MDP中，'Markov'是指action outcome只取决于当前state，与前序states无关。$$\n",
    "P(S_{t+1} = s^{'}|S_t=s_t,A_t=a_t, S_{t-1},A_{t-1},..., S_0) = P(S_{t+1} = s^{'}|S_t=s_t,A_t=a_t)$$\n",
    "\n",
    "2. MDP的定义：\\\n",
    "在随机的、完全可观察(fully observable)的环境中，transition model符合markovian的条件下，用可加的rewards做序列决策的过程。 \\\n",
    "<font color=green>完全可观察(fully observable)是指transition function和reward function已知</font> \n",
    "\n",
    "3. 用MDP分析问题所需的要素：\\\n",
    "① 一系列状态S，$s\\in S$ \\\n",
    "② 一系列行动A，$a\\in A$ \\\n",
    "③ 一个起始状态 \\\n",
    "④ 一个或者多个终止状态 \\\n",
    "⑤ rewards折扣因子$\\gamma$ \\\n",
    "⑥ 一个transition function $T(s, a, s^{'})=P(s^{'}|s, a)$ \\\n",
    "⑦ 一个reward function $R(s, a, s^{'})$\n",
    "\n",
    "4. 求解MDP问题的目标：求policy $\\pi(s)$ \\\n",
    "① 策略的质量由该策略导致的一系列环境状态s对应的回报r折现得到的效用的期望值$E(U|\\pi)$决定。（expected utility of the possible environment histories generated by that policy）$$\\begin{align} \n",
    "E(U|\\pi) & = E( {\\textstyle \\sum_{t=0}^{n}} R_t|\\pi) \\\\\n",
    "& = E( {\\textstyle \\sum_{t=0}^{n}} R(s_t, a_t, s_{t+1})|\\pi) \\\\\n",
    "& = E(R_0+\\gamma R_1+\\gamma ^2R_2+...+\\gamma ^nR_n|\\pi ) \\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "② 最优策略：$\\pi ^*=\\underset{\\pi}{argmax}\\ E(U|\\pi)$ \\ \n",
    "\n",
    "5. 求解MDP问题的解法：dynamic programming \\\n",
    "simplifying a problem by recursively breaking it into smaller pieces and remembering the optimal solutions to the pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41975b5f",
   "metadata": {},
   "source": [
    "### I.2 策略的稳定性\n",
    "1. 决策时限：MDP问题考察的时间长度可能是finite horizon或者infinite horizon的，两种情况下最优决策会有所不同。\n",
    "2. 在<font color=orange>**infinite horizon**</font>条件下做决策: 最优决策是<font color=blue>**稳定的(optimal policy is stationary)**</font> \\\n",
    "如果决策时间长度是infinite horizon的，那么最优策略只取决于状态s。\\\n",
    "<font color=red>**[rk's note]**</font> \\\n",
    "<font color=blue>由于reward可加，且$R_t=R(s_t, a_t, s_{t+1})$即当期reward由$s_t, a_t$和transition function决定。而transition model符合markovian，也就是$s_{t+1}$只取决于$s_t和a_t$。因此，给定transition function的条件下，reward $R_t$由$s_t和a_t$决定。 \\\n",
    "这意味着: \\\n",
    "i. policy只看当期s，不用考虑前序状态，因此$\\pi=\\pi(s_t)=P(a|s_t)$。 \\\n",
    "ii. 给定状态$s$，最优policy $\\pi^*(s)$是稳定分布，也就是只要s不变，policy的分布也不变。  </font>\n",
    "3. 在<font color=orange>**finite horizon**</font>条件下做决策: 最优决策是<font color=blue>**不稳定的(optimal policy is nonstationary)**</font>  \\\n",
    "如果决策时间长度是finite horizon的，那么最优策略不仅取决于状态s，还取决于剩余的time horizon。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e17f08e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e3a0446",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da64f08d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80869451",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "042e306a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cf394d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d03e7c30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb751892",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
