{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6c63ad",
   "metadata": {},
   "source": [
    "# MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e62207",
   "metadata": {},
   "source": [
    "## I. MDP\n",
    "### I.1 定义MDP\n",
    "1. ‘Markov’的含义：\\\n",
    "① 一般场景下，'Markov'是指给定当前状态，未来和过去的状态是独立的。\\\n",
    "② 在MDP中，'Markov'是指action outcome只取决于当前state，与前序states无关。$$\n",
    "P(S_{t+1} = s^{'}|S_t=s_t,A_t=a_t, S_{t-1},A_{t-1},..., S_0) = P(S_{t+1} = s^{'}|S_t=s_t,A_t=a_t)$$\n",
    "\n",
    "2. MDP的定义：\\\n",
    "在随机的、完全可观察(fully observable)的环境中，transition model符合markovian的条件下，用可加的rewards做序列决策的过程。 \\\n",
    "注意这个定义中的要素：\\\n",
    "① random environment: transition function有随机性 \\\n",
    "② <font color=green>fully observable environment是指transition function和reward function已知</font> \\\n",
    "③ markovian \\\n",
    "④ <font color=red>additive rewards: utility可以用rewards的其他函数形式来定义。但是如果utility要满足preference-independent，那么用sum of discounted rewards是唯一符合该假设的函数形式。</font> <font color=blue>[详见AI：a modern approach 4th ch17.1] </font>\n",
    "\n",
    "3. 用MDP分析问题所需的要素：\\\n",
    "① 一系列状态S，$s\\in S$ \\\n",
    "② 一系列行动A，$a\\in A$ \\\n",
    "③ 一个起始状态 \\\n",
    "④ 一个或者多个终止状态 \\\n",
    "⑤ rewards折扣因子$\\gamma$ \\\n",
    "⑥ 一个transition function $T(s, a, s^{'})=P(s^{'}|s, a)$ \\\n",
    "⑦ 一个reward function $R(s, a, s^{'})$\n",
    "\n",
    "4. 求解MDP问题的目标：求policy $\\pi(s)$ \\\n",
    "① 策略的质量由该策略导致的一系列环境状态s对应的回报r折现得到的效用的期望值$E(U|\\pi)$决定。（expected utility of the possible environment histories generated by that policy）$$\\begin{align} \n",
    "E(U|\\pi) & = E( {\\textstyle \\sum_{t=0}^{n}} R_t|\\pi) \\\\\n",
    "& = E( {\\textstyle \\sum_{t=0}^{n}} R(s_t, a_t, s_{t+1})|\\pi) \\\\\n",
    "& = E(R_0+\\gamma R_1+\\gamma ^2R_2+...+\\gamma ^nR_n|\\pi ) \\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "② 最优策略：$\\pi ^*=\\underset{\\pi}{argmax}\\ E(U|\\pi)$\n",
    "\n",
    "5. 求解MDP问题的解法：dynamic programming \\\n",
    "simplifying a problem by recursively breaking it into smaller pieces and remembering the optimal solutions to the pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d49cbd",
   "metadata": {},
   "source": [
    "### I.2 策略的稳定性\n",
    "1. 决策时限：MDP问题考察的时间长度可能是finite horizon或者infinite horizon的，两种情况下最优决策会有所不同。 \\\n",
    "<font color=green>finite horizon的意思是说，有一个固定的时间长度n，在这个时间之后的rewards不影响utility。比如，下围棋约定只能各走20子，20子之后，就算胜负扭转也不计入结果。</font>\n",
    "2. 在<font color=orange>**infinite horizon**</font>条件下做决策: 最优决策是<font color=blue>**稳定的(optimal policy is stationary)**</font> \\\n",
    "· 含义：<font color=purple>如果决策时间长度是infinite horizon的，那么最优策略只取决于状态s。</font> \\\n",
    "· 理解：<font color=red>**[rk's note]**</font> \\\n",
    "① 时间对utility的影响这时候是通过discount factor $\\gamma$来作用的，因此不同在函数中单独考虑time horizon。 \\\n",
    "② <font color=blue>由于reward可加，且$R_t=R(s_t, a_t, s_{t+1})$即当期reward由$s_t, a_t$和transition function决定。而transition model符合markovian，也就是$s_{t+1}$只取决于$s_t和a_t$。因此，给定transition function的条件下，reward $R_t$由$s_t和a_t$决定。这意味着: \\\n",
    "i. policy只看当期s，不用考虑前序状态，因此$\\pi=\\pi(s_t)=P(a|s_t)$。 \\\n",
    "ii. 给定状态$s$，最优policy $\\pi^*(s)$是稳定分布，也就是只要s不变，policy的分布也不变。  </font>\n",
    "3. 在<font color=orange>**finite horizon**</font>条件下做决策: 最优决策是<font color=blue>**不稳定的(optimal policy is nonstationary)**</font>  \\\n",
    "· 含义：<font color=purple>如果决策时间长度是finite horizon的，那么最优策略不仅取决于状态s，还取决于剩余的time horizon。</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee790a4",
   "metadata": {},
   "source": [
    "## II. Bellman equation\n",
    "1. 概念符号：\\\n",
    "① 给定策略$\\pi$，start state为s，start action为a，infinite horizon条件下的期望效用：$Q^{\\pi}(s, a)$\n",
    "$$Q^{\\pi}(s, a) =E(U|\\pi,s_0=s, a_0=a)  = E( {\\textstyle \\sum_{t=0}^{\\infty }} \\gamma ^tR_t|\\pi,s_0=s, a_0=a)$$\n",
    "② 给定策略$\\pi$，start state为s，infinite horizon条件下的期望效用：$V^{\\pi}(s)$ \n",
    "$$V^{\\pi}(s)=E(U|\\pi,s_0=s)  = E_AQ^{\\pi}(s, a)$$\n",
    "③ start state为s，start action为a，infinite horizon条件下，从下一步开始用最优策略达到的期望效用：$Q^{\\pi^*}(s, a)$，简记为：$Q^*(s, a)$ \n",
    "$$Q^{*}(s, a)=\\underset{\\pi}{max}\\ Q^{\\pi}(s, a)=\\underset{\\pi}{max}\\ E(U|\\pi, s_0=s, a_0=a)$$\n",
    "④ start state为s，infinite horizon条件下，用最优策略达到的期望效用：$V^{\\pi^*}(s)$，简记为：$V^*(s)$ \n",
    "$$V^*(s)=V^{\\pi^*}(s)=\\underset{\\pi}{max}\\ V^{\\pi}(s)=\\underset{\\pi}{max}\\ E(U|\\pi,s_0=s)$$\n",
    "⑤ infinite horizon条件下，用最优策略：$\\pi^*$ \n",
    "$$\\pi^*_s=\\underset{\\pi}{argmax}\\ V^{\\pi}(s)$$\n",
    "注1：<font color=red>虽然U的结果与起点时刻的state有关，但是当使用discounted utility，且infinite horizon时，$\\pi^*$与起点时刻的state无关。$$\\pi^*=\\pi^*_s,\\ \\forall s\\in S$$</font> \n",
    "注2：<font color=red>$\\pi^*=\\pi^*_s$表示start state为s时的整体策略，$\\pi^*(s)$表示当前state是s，当前步的最优策略。\n",
    "$$\\pi^*(s)=\\underset{a}{argmax}Q^{*}(s, a)$$</font> \\\n",
    "⑥ 相互关系 \\\n",
    "<font color=blue>$$\\begin{matrix}\n",
    " Q^{\\pi}(s, a) & \\overset{\\pi=\\pi^*}{\\rightarrow} & Q^{*}(s, a)\\\\\n",
    "|& & |\\\\\n",
    " {\\scriptsize E_{A\\sim \\pi}Q^{\\pi}(s, a)}   &  & {\\scriptsize E_{A\\sim \\pi^{*}}Q^{*}(s, a)}\\\\\n",
    "|& & |\\\\\n",
    " V^{\\pi}(s) & \\overset{\\pi=\\pi^*}{\\rightarrow} & V^{*}(s)\n",
    "\\end{matrix}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a763b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T06:41:09.722137Z",
     "start_time": "2023-12-03T06:41:09.703762Z"
    }
   },
   "source": [
    "2. recursive definition of utility/value \\\n",
    "① <font color=blue>$U_t=R_t+\\gamma U_{t+1}$</font> \\\n",
    "<font color=gray>证明：$$\\begin{align} \n",
    "U_t & = R_{t} + \\gamma R_{t+1} + \\gamma ^2 R_{t+2} + ... +\\gamma ^n R_{t+n} + ...\\\\\n",
    "& = R_t + \\gamma (R_{t+1} + \\gamma ^1 R_{t+2} + ... +\\gamma ^{n-1} R_{t+n} + ...)\\\\\n",
    "& = R_t+\\gamma U_{t+1}\n",
    "\\end{align}$$ </font>\n",
    "② <font color=blue>$Q^{\\pi }(s,a) = \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{\\pi }(s^{'})] $</font>  \\\n",
    "<font color=gray>证明：$$\\begin{align} \n",
    "Q^{\\pi }(s,a) & = E[R_{0} + \\gamma R_{1} + \\gamma ^2 R_{2} + ... +\\gamma ^n R_{n} + ...|\\pi, s_0=s,a_0=a]\\\\\n",
    "& = E[R(s,a,s_{1}) + \\gamma R(s_1,a_1,s_{2}) + ... +\\gamma ^n R(s_n,a_n,s_{n+1}) + ...|\\pi]\\\\\n",
    "& = \\sum_{s^{'}}^{}P(s^{'}|s,a) [R(s,a,s^{'})+ \\gamma E[R(s^{'},a_1,s_{2}) + ... +\\gamma ^{n-1} R(s_n,a_n,s_{n+1}) + ...|\\pi]] \\\\\n",
    "& = \\sum_{s^{'}}^{}P(s^{'}|s,a)[R(s,a,s^{'})+\\gamma V^{\\pi }(s^{'})] \\\\\n",
    "& = \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{\\pi }(s^{'})] \n",
    "\\end{align}$$ </font>\n",
    "③ <font color=blue>$Q^{*}(s,a) = \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{*}(s^{'})]$</font> \\\n",
    "<font color=gray>证明：代入$\\pi=\\pi^*$即可\n",
    "$$\\begin{align} Q^{*}(s,a) = \\underset{\\pi }{max}\\ Q^{\\pi }(s,a)= \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{*}(s^{'})]\n",
    "\\end{align}$$ </font>\n",
    "④ <font color=blue>$V^*(s)=\\underset{a}{max} Q^*(s, a)=\\underset{a}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{*}(s^{'})]$ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba06dfc1",
   "metadata": {},
   "source": [
    "3. Bellman equation \\\n",
    "<font color=blue>$$V^*(s)=\\underset{a}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{*}(s^{'})]$$</font>\n",
    "含义：如果agent选择最优策略，那么当前状态的效用等于各种可能得下期状态对应的当期回报和下期效用的折现值之和的加权平均。权重是用transition function确定的下期状态$s^{'}$发生概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef49cf0",
   "metadata": {},
   "source": [
    "## III. 求解Bellman equation的4种方法\n",
    "### III.1 value iteration\n",
    "1. 算法\n",
    "\n",
    "2. 收敛性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca014dd7",
   "metadata": {},
   "source": [
    "### III.2 policy iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd929f1",
   "metadata": {},
   "source": [
    "### III.3 linear programming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09cce6",
   "metadata": {},
   "source": [
    "### III.4 online algorithms for MDPs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c49fec9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a379864",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
