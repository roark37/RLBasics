{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea3b633",
   "metadata": {},
   "source": [
    "# MDP: markov decision process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beff702",
   "metadata": {},
   "source": [
    "## I. MDP\n",
    "### I.1 定义MDP\n",
    "1. **Markov property：**\\\n",
    "① 指给定当前状态，未来和过去的状态是独立的。\\\n",
    "② 在MDP中，'Markov'是指action outcome只取决于当前state，与前序states无关。$$\\begin{align}\n",
    "P(S_{t+1} = s^{'}|S_t=s_t,A_t=a_t, S_{t-1},A_{t-1},..., S_0) = P(S_{t+1} = s^{'}|S_t=s_t,A_t=a_t)\\\\\n",
    "\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "2. **MDP的定义：在完全可观察(fully observable)的、随机的环境中，transition model符合markovian的条件下，用可加的rewards做序列决策的过程。** \\\n",
    "注意这个定义中的要素：\\\n",
    "① <font color=green>fully observable environment是指transition function和reward function已知</font> \\\n",
    "② random environment: transition function有随机性 \\\n",
    "③ markovian transition function: $T(s, a, s^{'})=P(s^{'}|s, a)$ \\\n",
    "④ <font color=red>additive rewards: utility可以用rewards的其他函数形式来定义。但是如果utility要满足preference-independent，那么用sum of discounted rewards是唯一符合该假设的函数形式。</font> \\\n",
    "<font color=blue>[详见AI：a modern approach 4th ch17.1] </font>\n",
    "\n",
    "3. **用MDP分析问题所需的要素：**\\\n",
    "① 一系列状态S，$s\\in S$ \\\n",
    "② 一系列行动A，$a\\in A$ \\\n",
    "③ 一个起始状态 \\\n",
    "④ 一个或者多个终止状态 \\\n",
    "⑤ rewards折扣因子$\\gamma$ \\\n",
    "⑥ 一个transition function $T(s, a, s^{'})=P(s^{'}|s, a)$ \\\n",
    "⑦ 一个reward function $R(s, a, s^{'})$ \\\n",
    "<font color=orange>**图示: movements of an agent through a MDP:**</font> \\\n",
    "$$\\begin{align} \n",
    "& s_0\\overset{a_0}{\\rightarrow} s_1\\overset{a_1}{\\rightarrow} s_2\\overset{a_2}{\\rightarrow} s_3\\overset{a_3}{\\rightarrow}...\\\\\n",
    "& a_t\\sim P(a_t|s_t)=\\pi(s_t)\\\\ \n",
    "& s_t\\sim P(s_t|s_{t-1}, a_{t-1})=T(s_{t-1}, a_{t-1}, s_t) \\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "4. 求解MDP问题的目标：求policy $\\pi(s)$ \\\n",
    "① 策略的质量由该策略导致的一系列环境状态s对应的回报r折现得到的效用的期望值$E(U|\\pi)$决定。（expected utility of the possible environment histories generated by that policy）$$\\begin{align} \n",
    "E(U|\\pi) & = E( {\\textstyle \\sum_{t=0}^{n}} R_t|\\pi) \\\\\n",
    "& = E( {\\textstyle \\sum_{t=0}^{n}} R(s_t, a_t, s_{t+1})|\\pi) \\\\\n",
    "& = E(R_0+\\gamma R_1+\\gamma ^2R_2+...+\\gamma ^nR_n|\\pi ) \\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "② 最优策略：\n",
    "$$\\begin{align} \n",
    "\\pi ^*=\\underset{\\pi}{argmax}\\ E(U|\\pi) \\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "5. **MDP问题的解法：dynamic programming** \\\n",
    "simplifying a problem by recursively breaking it into smaller pieces and remembering the optimal solutions to the pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb742fd",
   "metadata": {},
   "source": [
    "### I.2 决策时限和策略的稳定性\n",
    "MDP问题考察的时间长度可能是finite horizon或者infinite horizon的，两种情况下最优决策会有所不同。 \\\n",
    "<font color=green>finite horizon的意思是说，有一个固定的时间长度n，在这个时间之后的rewards不影响utility。比如，下围棋约定只能各走20子，20子之后，就算胜负扭转也不计入结果。</font>\n",
    "1. 在<font color=orange>**infinite horizon**</font>条件下做决策: 最优决策是<font color=blue>**稳定的(optimal policy is stationary)**</font> \\\n",
    "· 含义：<font color=purple>如果决策时间长度是infinite horizon的，那么最优策略只取决于状态s。</font> \\\n",
    "· 理解：<font color=red>**[rk's note]**</font> \\\n",
    "① 时间对utility的影响这时候是通过discount factor $\\gamma$来作用的，因此不同在函数中单独考虑time horizon。 \\\n",
    "② <font color=blue>由于reward可加，且$R_t=R(s_t, a_t, s_{t+1})$即当期reward由$s_t, a_t$和transition function决定。而transition model符合markovian，也就是$s_{t+1}$只取决于$s_t和a_t$。因此，给定transition function的条件下，reward $R_t$由$s_t和a_t$决定。这意味着: \\\n",
    "i. policy只看当期s，不用考虑前序状态，因此$\\pi=\\pi(s_t)=P(a|s_t)$。 \\\n",
    "ii. 给定状态$s$，最优policy $\\pi^*(s)$是稳定分布，也就是只要s不变，policy的分布也不变。  </font> \n",
    "$$\n",
    "$$\n",
    "2. 在<font color=orange>**finite horizon**</font>条件下做决策: 最优决策是<font color=blue>**不稳定的(optimal policy is nonstationary)**</font>  \\\n",
    "· 含义：<font color=purple>如果决策时间长度是finite horizon的，那么最优策略不仅取决于状态s，还取决于剩余的time horizon。</font> \\\n",
    "<font color=red>注：要区分time horizon和agent的步数(number of timesteps)。infinite horizon并不意味着会走无限步。在infinite horizon条件下，agent一旦走到terminal state一样会停止，实际走的步数仍然是有限的。不过，如果$\\gamma=1$，那么可能出现最优决策是不走向terminal state，此时实际步数是无限的。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480935e",
   "metadata": {},
   "source": [
    "## II. Bellman equation\n",
    "1. 概念符号：\\\n",
    "① 给定策略$\\pi$，start state为s，start action为a，infinite horizon条件下的期望效用：\n",
    "$$\\begin{align} \n",
    "  Q^{\\pi}(s, a) =E(U|\\pi,s_0=s, a_0=a)  = E( {\\textstyle \\sum_{t=0}^{\\infty }} \\gamma ^tR_t|\\pi,s_0=s, a_0=a)\\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "② 给定策略$\\pi$，start state为s，infinite horizon条件下的期望效用：\n",
    "$$\\begin{align} \n",
    "V^{\\pi}(s)=E(U|\\pi,s_0=s)  = E_AQ^{\\pi}(s, a)\\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "③ start state为s，start action为a，infinite horizon条件下，从下一步开始用最优策略达到的期望效用：\n",
    "$$\\begin{align} \n",
    "Q^{*}(s, a)=\\underset{\\pi}{max}\\ Q^{\\pi}(s, a)=\\underset{\\pi}{max}\\ E(U|\\pi, s_0=s, a_0=a)\\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "④ start state为s，infinite horizon条件下，用最优策略达到的期望效用：\n",
    "$$\\begin{align} \n",
    "V^*(s)=V^{\\pi^*}(s)=\\underset{\\pi}{max}\\ V^{\\pi}(s)=\\underset{\\pi}{max}\\ E(U|\\pi,s_0=s)\\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "⑤ infinite horizon条件下，用最优策略：\n",
    "$$\\begin{align} \n",
    "\\pi^*_s=\\underset{\\pi}{argmax}\\ V^{\\pi}(s)\\\\\n",
    "\\end{align}$$\n",
    "注1：<font color=green>虽然U的结果与起点时刻的state有关，但是当使用discounted utility，且infinite horizon时，$\\pi^*$与起点时刻的state无关。$$\\pi^*=\\pi^*_s,\\ \\forall s\\in S$$</font> \n",
    "注2：<font color=green>$\\pi^*=\\pi^*_s$表示start state为s时的整体策略，$\\pi^*(s)$表示当前state是s，当前步的最优策略。\n",
    "$$\\begin{align} \n",
    "\\pi^*(s)=\\underset{a}{argmax}Q^{*}(s, a)\\\\\n",
    "\\end{align}$$</font> \\\n",
    "⑥ 相互关系 \\\n",
    "<font color=blue>$$\\begin{matrix}\n",
    " Q^{\\pi}(s, a) & \\overset{\\pi=\\pi^*}{\\rightarrow} & Q^{*}(s, a)\\\\\n",
    "|& & |\\\\\n",
    " {\\scriptsize E_{A\\sim \\pi}Q^{\\pi}(s, a)}   &  & {\\scriptsize E_{A\\sim \\pi^{*}}Q^{*}(s, a)}\\\\\n",
    "|& & |\\\\\n",
    " V^{\\pi}(s) & \\overset{\\pi=\\pi^*}{\\rightarrow} & V^{*}(s)\n",
    "\\end{matrix}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435cb6f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T06:41:09.722137Z",
     "start_time": "2023-12-03T06:41:09.703762Z"
    }
   },
   "source": [
    "2. recursive definition of utility/value \\\n",
    "① <font color=blue>$U_t=R_t+\\gamma U_{t+1}$</font> \\\n",
    "<font color=gray>证明：$$\\begin{align} \n",
    "U_t & = R_{t} + \\gamma R_{t+1} + \\gamma ^2 R_{t+2} + ... +\\gamma ^n R_{t+n} + ...\\\\\n",
    "& = R_t + \\gamma (R_{t+1} + \\gamma ^1 R_{t+2} + ... +\\gamma ^{n-1} R_{t+n} + ...)\\\\\n",
    "& = R_t+\\gamma U_{t+1}\n",
    "\\end{align}$$ </font>\n",
    "② <font color=blue>$Q^{\\pi }(s,a) = \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{\\pi }(s^{'})] $</font>  \\\n",
    "<font color=gray>证明：$$\\begin{align} \n",
    "Q^{\\pi }(s,a) & = E[R_{0} + \\gamma R_{1} + \\gamma ^2 R_{2} + ... +\\gamma ^n R_{n} + ...|\\pi, s_0=s,a_0=a]\\\\\n",
    "& = E[R(s,a,s_{1}) + \\gamma R(s_1,a_1,s_{2}) + ... +\\gamma ^n R(s_n,a_n,s_{n+1}) + ...|\\pi]\\\\\n",
    "& = \\sum_{s^{'}}^{}P(s^{'}|s,a) [R(s,a,s^{'})+ \\gamma E[R(s^{'},a_1,s_{2}) + ... +\\gamma ^{n-1} R(s_n,a_n,s_{n+1}) + ...|\\pi]] \\\\\n",
    "& = \\sum_{s^{'}}^{}P(s^{'}|s,a)[R(s,a,s^{'})+\\gamma V^{\\pi }(s^{'})] \\\\\n",
    "& = \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{\\pi }(s^{'})] \n",
    "\\end{align}$$ </font>\n",
    "③ <font color=blue>$Q^{*}(s,a) = \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{*}(s^{'})]$</font> \\\n",
    "<font color=gray>证明：代入$\\pi=\\pi^*$即可\n",
    "$$\\begin{align} Q^{*}(s,a) = \\underset{\\pi }{max}\\ Q^{\\pi }(s,a)= \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{*}(s^{'})]\n",
    "\\end{align}$$ </font>\n",
    "④ <font color=blue>$V^{\\pi}(s)=\\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,a,s^{'})+\\gamma V^{\\pi }(s^{'})]\n",
    "$ </font> \\\n",
    "<font color=gray>证明：$$V^{\\pi}(s)=Q^{\\pi}(s, a=\\pi(s))$$\n",
    "⑤ <font color=blue>$V^*(s)=\\underset{a}{max} Q^*(s, a)=\\underset{a}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{*}(s^{'})]$ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4cd5d7",
   "metadata": {},
   "source": [
    "3. **Bellman equation** \\\n",
    "<font color=blue>$$V^*(s)=\\underset{a}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V^{*}(s^{'})]$$</font>\n",
    "含义：如果agent选择最优策略，那么当前状态的效用等于当期回报的加权平均和下期效用折现值的加权平均之和。他们使用的加权权重都是由transition function决定的下期状态$s^{'}$发生概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b4f95",
   "metadata": {},
   "source": [
    "## III. 求解Bellman equation的4种方法\n",
    "### III.1 value iteration\n",
    "#### III.1.1 算法\n",
    "1. **value iteration**: \\\n",
    "已知transition model $T(s,a,s^{'})$，rewards $R(s,a,s^{'})$，收敛精度$\\epsilon$，discount $\\gamma$ \\\n",
    "初始化$V_0(s)=0$ \\\n",
    "迭代: \\\n",
    "$$\\begin{align} \n",
    "repeat:&&\\\\\n",
    "&\\delta=0 \\ &\\\\\n",
    "&for\\ s\\ in\\ S:&\\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ \\ V_{k+1}(s) \\leftarrow \\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V_k(s^{'})]\\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ \\ if\\ \\delta < |V_{k+1}(s)-V_{k}(s)|,\\ then \\ \\delta = |V_{k+1}(s)-V_{k}(s)|\\\\\n",
    "until:&\\delta<\\epsilon\\frac{(1-\\gamma )}{\\gamma}&\n",
    "\\end{align}$$\n",
    "2. <font color=blue>**policy extraction**: 当算法收敛后，得到$V^*(s)$，再用下式得到最优策略$\\pi^*(s)$\\\n",
    "$$\\begin{align} \n",
    "\\pi^*(s)& =\\underset{a\\in A}{argmax}Q^{*}(s, a)\\\\\n",
    "& =\\underset{a\\in A}{argmax} \\sum_{s^{'}}P(s^{'}|s, a)[R(s, a, s^{'})+\\gamma V^*(s^{'})] \n",
    "\\end{align}$$</font>\n",
    "3. <font color=red>每次iteration</font>的复杂度：$O(|S|^2|A|)$ \\\n",
    "分析：每个for循环内部的一轮计算是$O(|S||A|)$，每次iteration要做$|S|$次for循环。所以每次iteration的复杂度是$O(|S|^2|A|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8bd88",
   "metadata": {},
   "source": [
    "#### III.1.2 收敛性：value iteration会收敛到唯一最优解\n",
    "<font color=green>思路：利用Bellman equation满足contraction定义的特征，再用contraction的性质得到收敛的结论</font>\n",
    "1. <font color=blue>**contraction的定义和性质**</font> \\\n",
    "· **定义**：如果单变量函数f(x)满足:$$distance(f(a), f(b))\\le \\gamma * distance(a, b)，0\\le \\gamma<1$$则称f(x)是一个contraction。\\\n",
    "· **性质**：\\\n",
    "① <font color=brown>contraction最多只有一个固定的收敛点。序列: $x, f(x), f(f(x)), ...$会收敛到该点。</font> \\\n",
    "<font color=gray>简证：如果有两个的话，那么会向两个位置收敛，就无法满足distance缩小的条件了。</font>\\\n",
    "② <font color=brown>当$x=f(x)$时，就达到了收敛点。</font> \\\n",
    "· <font color=gray>eg: f(x)=x/2是contraction。当x=0时，收敛点是0，$x=0时，x=f(0)=0$。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7a5bd",
   "metadata": {},
   "source": [
    "2. **证明1：以max norm为distance measure时，bellman operator是一个contraction。**\\\n",
    "分析：\\\n",
    "① max norm：将$V$看做有$|S|$个元素的vector，则其max norm可以表示为：$$\\left \\| V \\right \\|_{\\infty} = \\underset{s\\in S}{max} |V(s)|$$\n",
    "② Bellman update:$$\\begin{align} \n",
    "& V_{k+1}(s) \\leftarrow \\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V_k(s^{'})]\\\\\n",
    "& 取B(V(s))=\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})]\\\\\n",
    "& B(V)=\\begin{pmatrix}\n",
    "B(V(s_1)) \\\\\n",
    "B(V(s_2)) \\\\\n",
    "... \\\\\n",
    "B(V(s_m))\n",
    "\\end{pmatrix}, s\\in S=\\{s_1, s_2, ..., s_m\\}  \\\\\n",
    "& B称为Bellman\\ operator \\\\\n",
    "\\end{align}$$\n",
    "③ 定义Bellman operator: B(V) \n",
    "$$\\begin{align} \n",
    "& 取B(V(s))=\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})], s\\in S=\\{s_1, s_2, ..., s_m\\} \\\\\n",
    "& B(V)=\\begin{pmatrix}\n",
    "B(V(s_1)) \\\\\n",
    "B(V(s_2)) \\\\\n",
    "... \\\\\n",
    "B(V(s_m))\n",
    "\\end{pmatrix} =\\begin{pmatrix}\n",
    "\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s_1,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})] \\\\\n",
    "\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s_2,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})] \\\\\n",
    "... \\\\\n",
    "\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s_m,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})]\n",
    "\\end{pmatrix}\n",
    "\\end{align}$$\n",
    "④ 需要证明：$$\\left \\| B(V)-B(\\tilde V) \\right \\|_{\\infty} \\le \\gamma \\left \\| V-\\tilde V \\right \\| _{\\infty}$$\n",
    "<font color=green>思路：$$\\begin{align} \n",
    "& \\because \\left \\| B(V)-B(\\tilde V) \\right \\|_{\\infty}=\\underset{s\\in S}{max} \\left | B(V(s))-B(\\tilde V(s)) \\right | \\\\\n",
    "& \\therefore 只要证明，对\\forall s都有\\left | B(V(s))-B(\\tilde V(s)) \\right |\\le \\gamma \\left \\| V-\\tilde V \\right \\| _{\\infty}即可 \\\\\n",
    "\\end{align}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b85d1",
   "metadata": {},
   "source": [
    "2. 证明1（续） \\\n",
    "⑤ 证明过程：\\\n",
    "利用任意不等式都有的性质：$$|max\\ f(x)-max\\ h(x)|\\le max|f(x)-h(x)|$$\n",
    "$$\\begin{align} \n",
    "& |BV(s)-B\\tilde V(s)| \\\\\n",
    "& =\\left|\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})]-\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma \\tilde V(s^{'})]\\right|\\\\\n",
    "& \\le \\underset{a\\in A}{max}\\left |\\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})]-\\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma \\tilde V(s^{'})]  \\right | \\\\\n",
    "& = \\underset{a\\in A}{max}\\left |\\gamma \\sum_{s^{'}}^{}T(s,a,s^{'})V(s^{'}) - \\gamma \\sum_{s^{'}}^{}T(s,a,s^{'})\\tilde V(s^{'})\\right | \\\\\n",
    "& = \\gamma*\\underset{a\\in A}{max}\\left |\\sum_{s^{'}}^{}T(s,a,s^{'})[V(s^{'})-\\tilde V(s^{'})]  \\right | \\\\\n",
    "& \\le \\gamma*\\underset{a\\in A}{max}\\left |\\sum_{s^{'}}^{}T(s,a,s^{'})\\underset {s^{'}}{max}|V(s^{'})-\\tilde V(s^{'})| \\right | \\\\\n",
    "& = \\gamma*\\underset {s^{'}}{max}\\left | V(s^{'})-\\tilde V(s^{'}) \\right |=\\gamma \\left \\| V-\\tilde V \\right \\|_{\\infty}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0680fe",
   "metadata": {},
   "source": [
    "3. **证明2：value iteration会收敛到唯一最优解** \\\n",
    "证明：\\\n",
    "Bellman equation迭代的过程相当于iterated sequence：$x, f(x), f(f(x)), ...$。\\\n",
    "根据contraction的性质，iterated sequence会收敛到固定值，且当$x=f(x)$时到达该收敛点。\\\n",
    "因此，$B(V)$会收敛到唯一的固定值：$$B(V)=\\begin{pmatrix}\n",
    "B(V(s_1)) \\\\\n",
    "B(V(s_2)) \\\\\n",
    "... \\\\\n",
    "B(V(s_m))\n",
    "\\end{pmatrix} \\rightarrow \\begin{pmatrix}\n",
    "V^*(s_1) \\\\\n",
    "V^*(s_2) \\\\\n",
    "... \\\\\n",
    "V^*(s_m)\n",
    "\\end{pmatrix}=V^* \\\\\n",
    "且此时，V^*=B(V^*)=\\begin{pmatrix}\n",
    "B(V^*(s_1)) \\\\\n",
    "B(V^*(s_2)) \\\\\n",
    "... \\\\\n",
    "B(V^*(s_m))\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a2717",
   "metadata": {},
   "source": [
    "4. 收敛速度：rate of convergence \\\n",
    "① <font color=blue>**value iteration converges exponentially fast.**</font> \\\n",
    "分析：$取error_k = \\left \\| B(V_{k})-B(V^*) \\right \\|_{\\infty}$\n",
    "$$\\begin{align} \n",
    "\\because \\left \\| B(V_{k+1})-B(V^*) \\right \\|_{\\infty} & \\le \\gamma *\\left \\| V_{k+1}-V^* \\right \\|_{\\infty} \\\\\n",
    "& = \\gamma *\\left \\| B(V_{k})-V^* \\right \\|_{\\infty} \n",
    "\\end{align}$$\n",
    "$$\\begin{align} \n",
    "\\therefore \\frac{\\left \\| B(V_{k+1})-B(V^*) \\right \\|_{\\infty}}{\\left \\| B(V_{k})-B(V^*) \\right \\|_{\\infty}}\\le \\gamma \\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "② 要让收敛精度达到$\\epsilon$，则需要的迭代条件是$\\left \\| V_{k+1}-V_k \\right \\|_{\\infty} \\le \\epsilon (1-\\gamma)/\\gamma$。因为： \\\n",
    "$$if\\ \\left \\| V_{k+1}-V_k \\right \\|_{\\infty} \\le \\epsilon (1-\\gamma)/\\gamma,\\ \\ then\\ \\left \\| V_{k+1}-V^* \\right \\|_{\\infty} \\le \\epsilon$$\n",
    "分析：(略)<font color=blue>[详见AI：a modern approach 4th ch17.2 page575] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4de5e2",
   "metadata": {},
   "source": [
    "### III.2 policy iteration\n",
    "出发点：策略的收敛通常比value收敛发生的早很多。而bellman equation本身的目标就是找到最优策略。因此一个思路是直接求策略，而不是先求optimal value再得到对应的策略。\n",
    "#### III.1.1 算法\n",
    "1. **思路**: <font color=green>每次迭代先做policy evaluation，再做policy improvement，直到policy evaluation得到的utility没有变化，也即utility收敛到了contraction的收敛点。此时policy extraction得到的就是最优策略。</font>\\\n",
    "① <font color=blue>**policy evaluation**</font>：给定策略$\\pi_k$，对所有的states s计算value/utility:$$V^{\\pi_k}(s)=\\sum_{s^{'}}^{}T(s,\\pi_k(s),s^{'})[R(s,\\pi_k(s),s^{'})+\\gamma V^{\\pi_k}(s^{'})]$$\n",
    "② <font color=blue>**policy improvement**</font>：向前看一步，更新策略$$\\begin{align} \n",
    "\\pi_{k+1}(s) & \\leftarrow \\underset{a\\in A}{argmax}Q^{\\pi_k}(s, a)\\\\\n",
    "& =\\underset{a\\in A}{argmax} \\sum_{s^{'}}P(s^{'}|s, a)[R(s, a, s^{'})+\\gamma V^{\\pi_k}(s^{'})] \n",
    "\\end{align}$$\n",
    "2. **Q-value iteration算法**： \\\n",
    "已知transition model $T(s,a,s^{'})$，rewards $R(s,a,s^{'})$，收敛精度$\\epsilon$，discount $\\gamma$ \\\n",
    "初始化：$V_0^{\\pi}(s)=0$，任意初始化$\\pi_0$ \\\n",
    "迭代: \\\n",
    "$$\\begin{align} \n",
    "repeat:&\\\\\n",
    "& \\#\\ policy\\ evaluation,解|S|个方程|S|个未知数的线性方程组 \\\\\n",
    "&for\\ s\\ in\\ S: \\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ \\ V^{\\pi}(s)=\\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,a,s^{'})+\\gamma V^{\\pi}(s^{'})] \\\\\n",
    "& \\#\\ policy\\ improvement \\\\\n",
    "& for\\ s\\ in\\ S: \\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ \\ \\pi_{k+1}(s) \\leftarrow\\underset{a\\in A}{argmax} \\sum_{s^{'}}P(s^{'}|s, a)[R(s, a, s^{'})+\\gamma V^{\\pi_k}(s^{'})]\\\\\n",
    "until:&\\  \\pi\\ stable\\\\\n",
    "return:&\\ \\pi\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685aa875",
   "metadata": {},
   "source": [
    "3. 算法复杂度 \\\n",
    "· **policy evaluation** \\\n",
    "① 精确求解：直接求解线性方程组，那么正常情况下是$O(|S|^3)$。但如果利用状态转移函数的稀疏性，可以大幅降低复杂度。\\\n",
    "② 近似求解：用迭代法得到$V^{\\pi}$的近似估计值\n",
    "$$V^{\\pi}_{k+1}(s) \\leftarrow \\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,\\pi(s),s^{'})+\\gamma V^{\\pi}_k(s^{'})] \n",
    "$$\n",
    "单次迭代的复杂度是$O(|S|^2)$ \\\n",
    "· **policy improvement**: $O(|S|^2|A|)$ \\\n",
    "分析：每个for循环内部的一轮计算是$O(|S||A|)$，每次iteration要做$|S|$次for循环。所以每次iteration的复杂度是$O(|S|^2|A|)$ \\\n",
    "<font color=red>分析：直接看算法复杂度时，policy iteration的每次迭代复杂度是$O(|S|^2|A|)=O(|S|^2|A|)+O(|S|^2)$。从单次迭代的复杂度来看，它和value iteration在同一水平上。但policy iteration需要的总的迭代次数更少。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d425a",
   "metadata": {},
   "source": [
    "#### III.1.2 收敛性\n",
    "略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3800b584",
   "metadata": {},
   "source": [
    "### III.3 linear programming\n",
    "跳过"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08af454",
   "metadata": {},
   "source": [
    "### III.4 online algorithms for MDPs\n",
    "跳过"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
