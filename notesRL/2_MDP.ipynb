{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea3b633",
   "metadata": {},
   "source": [
    "# MDP: markov decision process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beff702",
   "metadata": {},
   "source": [
    "## I. MDP\n",
    "### I.1 定义MDP\n",
    "1. **Markov property：**\\\n",
    "① 指给定当前状态，未来和过去的状态是独立的。\\\n",
    "② 在MDP中，'Markov'是指action outcome只取决于当前state，与前序states无关。$$\\begin{align}\n",
    "P(S_{t+1} = s^{'}|S_t=s_t,A_t=a_t, S_{t-1},A_{t-1},..., S_0) = P(S_{t+1} = s^{'}|S_t=s_t,A_t=a_t)\\\\\n",
    "\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "2. **MDP的定义：在完全可观察(fully observable)的、随机的环境中，transition model符合markovian的条件下，用additive and discounted rewards (utility)做序列决策的过程。** \\\n",
    "注意这个定义中的要素：\\\n",
    "① <font color=blue>**fully observable environment**</font>: 指transition function和reward function已知 \\\n",
    "② <font color=blue>**random environment**</font>: transition function有随机性 \\\n",
    "③ <font color=red>**markovian transition function**</font>: $T(s, a, s^{'})=P(s^{'}|s, a)$ \\\n",
    "④ <font color=blue>**additive rewards**</font>: utility可以用rewards的其他函数形式来定义。而additive utility的优点是简单，且满足preference-independent。</font> <font color=orange>[preference independent的说明详见最后的附录]</font> \\\n",
    "⑤ <font color=blue>**discounted utility function**</font>: 在value iteration和policy iteration的收敛性质中起到的关键作用。<font color=orange>[详见后文的收敛性分析]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cfafc",
   "metadata": {},
   "source": [
    "3. **用MDP分析问题所需的要素：**\\\n",
    "(1)状态 \\\n",
    "① 状态集$\\mathcal{S}$，如果状态是离散且有限的，则$\\mathcal{S}=\\{s^{(1)},s^{(2)},...,s^{(K)} \\}$。 \\\n",
    "② 一个起始状态$s_0$\\\n",
    "③ 一个或多个终止状态 \\\n",
    "(2)动作：$a\\in \\mathcal{A} $ \\\n",
    "(3)策略：$\\pi$：$\\pi(a|s)=P_{\\pi}(a|s)=P_{\\pi}(A=a|S=s)$ \\\n",
    "(4)回报 \\\n",
    "① 回报函数(reward function)：$R_t=R(s_t, a_t, s_{t+1})$ \\\n",
    "② 回报的折扣因子：$\\gamma$ \\\n",
    "(5)状态转移函数(transition function)：$T(历史状态, a, 下一状态)$。<font color=blue>**马尔科夫条件下**</font>，状态转移函数可以表示为：$T(s_t, a_t, s_{t+1}) =P(s_{t+1}|s_t, a_t)$ \\\n",
    "<font color=orange>**图示: movements of an agent through a MDP:**</font> \\\n",
    "$$\\begin{align} \n",
    "& s_0\\overset{a_0}{\\rightarrow} s_1\\overset{a_1}{\\rightarrow} s_2\\overset{a_2}{\\rightarrow} s_3\\overset{a_3}{\\rightarrow}...\\\\\n",
    "& a_t\\sim P(a_t|s_t)=\\pi(s_t)\\\\ \n",
    "& s_t\\sim P(s_t|s_{t-1}, a_{t-1})=T(s_{t-1}, a_{t-1}, s_t) \\\\\n",
    "\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06112a13",
   "metadata": {},
   "source": [
    "4. 求解MDP问题的目标：求policy $\\pi(s)$ \\\n",
    "① 策略的质量由该策略导致的一系列环境状态s对应的回报r折现得到的效用的期望值$E(U|\\pi)$决定。（expected utility of the possible environment histories generated by that policy）$$\\begin{align} \n",
    "E(U|\\pi) & = E( {\\textstyle \\sum_{t=0}^{n}} R_t|\\pi) \\\\\n",
    "& = E( {\\textstyle \\sum_{t=0}^{n}} R(s_t, a_t, s_{t+1})|\\pi) \\\\\n",
    "& = E(R_0+\\gamma R_1+\\gamma ^2R_2+...+\\gamma ^nR_n|\\pi ) \\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "② 最优策略：\n",
    "$$\\begin{align} \n",
    "\\pi ^*=\\underset{\\pi}{argmax}\\ E(U|\\pi) \\\\\n",
    "\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb742fd",
   "metadata": {},
   "source": [
    "### I.2 决策时限和策略的稳定性\n",
    "MDP问题考察的时间长度可能是finite horizon或者infinite horizon的，两种情况下最优决策会有所不同。 \\\n",
    "<font color=green>finite horizon的意思是说，有一个固定的时间长度n，在这个时间之后的rewards不影响utility。比如，下围棋约定只能各走20子，20子之后，就算胜负扭转也不计入结果。</font>\n",
    "1. 在<font color=orange>**finite horizon**</font>条件下做决策: 最优决策是<font color=blue>**不稳定的(optimal policy is nonstationary)**</font>  \\\n",
    "· 含义：<font color=purple>如果决策时间长度是finite horizon的，那么最优策略不仅取决于状态s，还取决于剩余的time horizon。这里“不稳定”是指最优决策会受time horizon的变化而变化。</font> \\\n",
    "· 例子：当前状态离终点有两条路径，一条路径近但是路上有掉进坑(比如utility为$-100$的state)的风险；另一条路径远，但是路上没有掉进坑的风险。如果time horizon是无限的，那么最优选择是走原路。但如果time horizon短，没有足够时间走远路，那就只能冒风险走又掉进坑的可能性的那条路。\\\n",
    "<font color=red>注：要区分time horizon和agent的步数(number of timesteps)。infinite horizon并不意味着会走无限步。在infinite horizon条件下，agent一旦走到terminal state一样会停止，实际走的步数仍然是有限的。不过，如果$\\gamma=1$，那么可能出现最优决策是不走向terminal state，此时实际步数是无限的。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3f59b0",
   "metadata": {},
   "source": [
    "2. 在<font color=orange>**infinite horizon**</font>条件下做决策: 最优决策是<font color=blue>**稳定的(optimal policy is stationary)**</font> \\\n",
    "· 含义：<font color=purple>如果决策时间长度是infinite horizon的，那么最优action a只取决于当前状态s，与当前所处的time无关，也与起点时刻的state $s_0$无关。</font>这也意味着$\\pi^*$与$s_0$无关。当然具体的action sequence和他们对应的实际得到的utility结果与起点时刻的state有关。 \\\n",
    "$$\\pi^*=\\pi^*_s,\\ \\forall s\\in S$$\n",
    "· 证明：<font color=blue>可以用bellman operator是contraction function的性质证明。参考后面的policy iteration的收敛性证明。</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480935e",
   "metadata": {},
   "source": [
    "## II. Bellman equation\n",
    "### II.1 概念符号\n",
    "① 给定策略$\\pi$，start state为s，start action为a，期望效用：\n",
    "$$\\begin{align} \n",
    "  Q^{\\pi}(s, a) =E(U|\\pi,s_0=s, a_0=a)  = E( {\\textstyle \\sum_{t=0}^{n}} \\gamma ^tR_t|\\pi,s_0=s, a_0=a)\\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "② 给定策略$\\pi$，start state为s，期望效用：\n",
    "$$\\begin{align} \n",
    "V^{\\pi}(s)=E(U|\\pi,s_0=s)  = E_AQ^{\\pi}(s, a)\\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "③ start state为s，start action为a，从下一步开始用最优策略达到的期望效用：\n",
    "$$\\begin{align} \n",
    "Q^{*}(s, a)=\\underset{\\pi}{max}\\ Q^{\\pi}(s, a)=\\underset{\\pi}{max}\\ E(U|\\pi, s_0=s, a_0=a)\\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "④ start state为s，用最优策略达到的期望效用：\n",
    "$$\\begin{align} \n",
    "V^*(s)=V^{\\pi^*}(s)=\\underset{\\pi}{max}\\ V^{\\pi}(s)=\\underset{\\pi}{max}\\ E(U|\\pi,s_0=s)\\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "⑤ 最优策略：\n",
    "$$\n",
    "\\pi^*=\\underset{\\pi}{argmax}\\ V^{\\pi}(s), \\forall s\\in \\mathcal{S}\n",
    "$$\n",
    "注1：<font color=green>$\\pi^*=\\pi^*_s$表示start state为s时的整体策略，$\\pi^*(s)$表示当前state是s，当前步的最优策略。\n",
    "$$\\begin{align} \n",
    "\\pi^*(s)=\\underset{a}{argmax}Q^{*}(s, a)\\\\\n",
    "\\end{align}$$</font> \\\n",
    "⑥ 相互关系 \\\n",
    "<font color=blue>$$\\begin{matrix}\n",
    " Q^{\\pi}(s, a) & \\overset{\\pi=\\pi^*}{\\rightarrow} & Q^{*}(s, a)\\\\\n",
    "|& & |\\\\\n",
    " {\\scriptsize E_{A\\sim \\pi}Q^{\\pi}(s, a)}   &  & {\\scriptsize E_{A\\sim \\pi^{*}}Q^{*}(s, a)}\\\\\n",
    "|& & |\\\\\n",
    " V^{\\pi}(s) & \\overset{\\pi=\\pi^*}{\\rightarrow} & V^{*}(s)\n",
    "\\end{matrix}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6701862f",
   "metadata": {},
   "source": [
    "### II.2 效用的迭代形式\n",
    "#### II.2.1 Reward function和Utility\n",
    "1. 已知(a, s)：此时只考虑下一状态的分布\n",
    "$$\n",
    "\\begin{align}\n",
    "E(R|a, s) & = E_{T}(R|s, a)   \\\\\n",
    "& = E_{P(s^{'}|s, a)}(R|s, a)   \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}} P(s^{'}|s, a)R(s, a, s^{'})\n",
    "\\end{align}\n",
    "$$\n",
    "2. 已知(s)：此时同时考虑action的分布和下一状态的分布 \n",
    "$$\n",
    "\\begin{align}\n",
    "E(R|s) & = E_{a\\sim A}[E(R|s, a)]   \\\\\n",
    "& = E_{a\\sim A}[\\sum_{s^{'}\\in \\mathcal{S}} P(s^{'}|s, a)R(s, a, s^{'})] \\\\\n",
    "& = \\sum _{a\\in \\mathcal{A}}\\pi (a|s)\\sum_{s^{'}\\in \\mathcal{S}} P(s^{'}|s, a)R(s, a, s^{'})\n",
    "\\end{align} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82016422",
   "metadata": {},
   "source": [
    "3. 效用的迭代形式 \\\n",
    "(1)time horizon有限的条件下\n",
    "$$\n",
    "\\begin{align}\n",
    "U_t\n",
    "& =R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+...+\\gamma^{n} R_{t+n} \\\\\n",
    "& =R_t+\\gamma \\left [ R_{t+1}+\\gamma R_{t+2}+...+ \\gamma^{n-1} R_{t+n} \\right ] \\\\\n",
    "& =R_t+\\gamma U_{t+1}\n",
    "\\end{align} \n",
    "$$\n",
    "(2)time horizon无限的条件下\n",
    "$$\n",
    "\\begin{align}\n",
    "U_t\n",
    "& =R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+...+\\gamma^{n} R_{t+n}+... \\\\\n",
    "& =R_t+\\gamma \\left [ R_{t+1}+\\gamma R_{t+2}+...+ \\gamma^{n-1} R_{t+n}+...  \\right ] \\\\\n",
    "& =R_t+\\gamma U_{t+1}\n",
    "\\end{align} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6327d5c8",
   "metadata": {},
   "source": [
    "### II.3 贝尔曼期望方程 [以time horizon无限为条件]\n",
    "<font color=red>**本质：time horizon无限的条件下，求Utility的期望值**</font>\n",
    "#### II.3.1 性质\n",
    "1. <font color=orange>**time horizon无限的条件下，期望效用与starting time无关。**</font>\n",
    "$$\n",
    "\\color{orange}  {E(U_{t=i}|s) =E(U_{t=j}|s)=E(U|s) , 其中i\\ne j}\n",
    "$$\n",
    "<font color=grey>证明：只要证明$E(U_{0}|s) =E(U_{t}|s)$即可 \\\n",
    "$$\n",
    "\\begin{align}\n",
    "E(U_0|s)\n",
    "& = E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s) \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'}) +\\gamma E(U_1|s^{'}) \\right ] \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)R(s, a, s^{'}) +\\gamma \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)E(U_1|s^{'})  \\\\\n",
    "E(U_t|s)\n",
    "& = E(R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+...+\\gamma^{n} R_{t+n}+...|s) \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U_{t+1}|s^{'}) \\right ] \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)R(s, a, s^{'}) +\\gamma \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)E(U_{t+1}|s^{'})  \\\\\n",
    "\\end{align} \n",
    "$$\n",
    "上面两个式子中展开的第一项相同，同样的道理继续展开他们各自对应的$E(U_1|s^{'})$和$E(U_{t+1}|s^{'})$，在infinite horizon条件下，只要reward function、策略$\\pi$和状态转移函数相同，那么后面所有展开的结果必然相同。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e7ff7",
   "metadata": {},
   "source": [
    "#### II.3.2 状态价值函数和行为价值函数的贝尔曼方程形式\n",
    "1. <font color=blue>**time horizon无限的条件下，求状态价值函数：** </font>\n",
    "$$\n",
    "\\color{Blue}  {V^{\\pi}(s)  = E(U|s) = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma V^{\\pi}(s^{'}) \\right ]} \n",
    "$$\n",
    "证明：\\\n",
    "(1)已知s，求效用的期望\n",
    "$$\n",
    "\\begin{align}\n",
    "E(U_0|s)\n",
    "& = E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s) \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s,a)\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s,a,s^{'})\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E\\left ( R_1+\\gamma R_{2}+...+\\gamma^{n-1} R_{n}+...|s, a, s^{'} \\right )  \\right ] \\\\ \n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U_1|S_0=s, A_0=a, S_1=s^{'}) \\right ] \\\\\n",
    "& \\because 马尔科夫条件和reward函数形式决定了U_1不受S_0和A_0取值的影响 \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U_1|s^{'}) \\right ]\n",
    "\\end{align} \n",
    "$$\n",
    "**(2)time horizon无限的条件下，状态价值函数的递归形式**\n",
    "$$\n",
    "\\begin{align}\n",
    "\\because E(U_t|s) & =E(U|s), E(U_{t+1}|s^{'})=E(U|s^{'})\\\\\n",
    "\\therefore E(U|s) & = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U|s^{'}) \\right ]\\\\\n",
    "\\therefore V^{\\pi}(s) & = E(U|s) = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma V^{\\pi}(s^{'}) \\right ]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b574a",
   "metadata": {},
   "source": [
    "2. <font color=blue>**time horizon无限的条件下，求行为价值函数：** </font>\n",
    "$$\n",
    "\\color{Blue} {\n",
    "\\begin{align}\n",
    "Q^{\\pi}(s,a)  = E(U|s,a) & = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma V^{\\pi}(s^{'}) \\right ] \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma \\sum_{a^{'}\\in \\mathcal{A}}\\pi(a^{'}|s^{'})Q^{\\pi}(s^{'},a^{'}) \\right ] \n",
    "\\end{align}\n",
    "}\n",
    "$$\n",
    "证明：\\\n",
    "(1)已知(s,a),求效用的期望\n",
    "$$\n",
    "\\begin{align}\n",
    "E(U_0|s,a)\n",
    "& = E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s,a) \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s,a,s^{'})\\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E\\left ( R_1+\\gamma R_{2}+...+\\gamma^{n-1} R_{n}+...|s, a, s^{'} \\right )  \\right ] \\\\ \n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U_1|s, a, s^{'}) \\right ] \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U_1|s^{'}) \\right ]\n",
    "\\end{align}\n",
    "$$\n",
    "**(2)time horizon无限的条件下，行为价值函数的递归形式**\n",
    "$$\n",
    "\\begin{align}\n",
    "\\because E(U_t|s,a) & = E(U|s,a), E(U_{t+1}|s^{'})=E(U|s^{'}) \\\\\n",
    "\\therefore E(U|s,a) & = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U|s^{'}) \\right ] \\\\\n",
    "\\therefore Q^{\\pi}(s,a) & =E(U|s,a) =\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma V^{\\pi}(s^{'}) \\right ]\n",
    "\\end{align}\n",
    "$$\n",
    "(3)转换成贝尔曼期望方程的形式：\n",
    "$$\n",
    "\\begin{align}\n",
    "Q^{\\pi}(s,a) & = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U|s^{'}) \\right ] \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma \\sum_{a^{'}\\in \\mathcal{A}}\\pi(a^{'}|s^{'})E(U|s^{'},a^{'}) \\right ] \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma \\sum_{a^{'}\\in \\mathcal{A}}\\pi(a^{'}|s^{'})Q^{\\pi}(s^{'},a^{'}) \\right ] \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da36e9",
   "metadata": {},
   "source": [
    "#### II.3.3 矩阵法求解贝尔曼方程\n",
    "1. 假设一共有K种状态，每种状态的期望效用可以表示为：\n",
    "$$\n",
    "\\begin{align}\n",
    "V^{\\pi}(s^{k}) & = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{i}\\in \\mathcal{S}}P(s^{i}|s, a)\\left [ R(s, a, s^{i})+\\gamma V^{\\pi}(s^{i}) \\right ] \\\\\n",
    "& = ER(s^{k}) + \\gamma \\sum _{s^{i}\\in \\mathcal{S}}P(s^{i}|s)V^{\\pi }(s^{i}) \\\\\n",
    "& k\\in {1, 2, ..., K}\n",
    "\\end{align}\n",
    "$$\n",
    "2. 用线性方程组方法求解：\\\n",
    "上式可以展开为：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "V(s^{1}) \\\\\n",
    "V(s^{2}) \\\\\n",
    "... \\\\\n",
    "V(s^{K})\n",
    "\\end{bmatrix}=E\\begin{bmatrix}\n",
    "r(s^{1}) \\\\\n",
    "r(s^{2}) \\\\\n",
    "... \\\\\n",
    "r(s^{K})\n",
    "\\end{bmatrix} + \\gamma \\begin{bmatrix}\n",
    "p(s^{1}|s^{1})  & p(s^{2}|s^{1}) & ... & p(s^{K}|s^{1})\\\\\n",
    "p(s^{1}|s^{2})  & p(s^{2}|s^{2}) & ... & p(s^{K}|s^{2})\\\\\n",
    " ... &  &  & \\\\\n",
    "p(s^{1}|s^{K})  & p(s^{2}|s^{K}) & ... & p(s^{K}|s^{K})\\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "V(s^{1})\\\\\n",
    "V(s^{2}) \\\\\n",
    "... \\\\\n",
    "V(s^{K})\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "求解线性方程组：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{V} & = \\mathcal{R} +\\gamma \\mathcal{PV} \\\\\n",
    "(\\mathcal{I-\\gamma P})\\mathcal{V}& =\\mathcal{R}\\\\\n",
    "\\mathcal{V} & = (\\mathcal{I-\\gamma P})^{-1}\\mathcal{R}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7122afb",
   "metadata": {},
   "source": [
    "3. **解的性质**：<font color=blue>**根据线性方程组的特征，n个方程求n个未知数，解存在且唯一。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b4f95",
   "metadata": {},
   "source": [
    "## III. 动态规划法解Bellman optimal equation\n",
    "### III.1 value iteration\n",
    "#### III.1.1 算法\n",
    "<font color=red>**理解要点：要从数学优化方法的角度来理解value iteration，不要从每一步迭代找到局部最优的角度，因为value iteration每一步迭代得到的$V_{k}(s)$都不是真的state s的value，而是数学优化方法的中间结果。（这一点和policy iteration不同，后者每次policy evaluation的结果都有具体的$\\pi$与之对应。）但迭代收敛后的$V^{*}$是真的最优state value。**</font>\n",
    "1. **value iteration**: \\\n",
    "已知transition model $T(s,a,s^{'})$，rewards $R(s,a,s^{'})$，收敛精度$\\epsilon$，discount $\\gamma$ \\\n",
    "初始化$V_0(s)=0$ \\\n",
    "迭代: \\\n",
    "$$\\begin{align} \n",
    "repeat:&&\\\\\n",
    "&\\delta=0 \\ &\\\\\n",
    "&for\\ s\\ in\\ S:&\\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ \\ V_{k+1}(s) \\leftarrow \\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V_k(s^{'})]\\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ \\ if\\ \\delta < |V_{k+1}(s)-V_{k}(s)|,\\ then \\ \\delta = |V_{k+1}(s)-V_{k}(s)|\\\\\n",
    "until:&\\delta<\\epsilon\\frac{(1-\\gamma )}{\\gamma}&\n",
    "\\end{align}$$\n",
    "<font color=red>**注：**下标表示time horizon，不是迭代次数的index。迭代条件$\\delta < |V_{k+1}(s)-V_{k}(s)|$虽然有下标，但是最终收敛的时候$V^*(s)=V_k(s), k\\rightarrow \\infty$，即在unlimited time horizon条件下，会收敛到$V^*(s)$。</font>\n",
    "2. <font color=blue>**policy extraction**: 当算法收敛后，得到$V^*(s)$，再用下式得到最优策略$\\pi^*(s)$\\\n",
    "$$\\begin{align} \n",
    "\\pi^*(s)& =\\underset{a\\in A}{argmax}Q^{*}(s, a)\\\\\n",
    "& =\\underset{a\\in A}{argmax} \\sum_{s^{'}}P(s^{'}|s, a)[R(s, a, s^{'})+\\gamma V^*(s^{'})] \n",
    "\\end{align}$$</font>\n",
    "3. <font color=red>每次iteration</font>的复杂度：$O(|S|^2|A|)$ \\\n",
    "分析：每个for循环内部的一轮计算是$O(|S||A|)$，每次iteration要做$|S|$次for循环。所以每次iteration的复杂度是$O(|S|^2|A|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8bd88",
   "metadata": {},
   "source": [
    "#### III.1.2 收敛性：value iteration会收敛到唯一最优解\n",
    "<font color=green>思路：利用Bellman equation满足contraction定义的特征，再用contraction的性质得到收敛的结论</font>\n",
    "1. <font color=blue>**contraction的定义和性质**</font> \\\n",
    "· **定义**：如果单变量函数f(x)满足:$$distance(f(a), f(b))\\le \\gamma * distance(a, b)，0\\le \\gamma<1$$则称f(x)是一个contraction。\\\n",
    "· **性质**：\\\n",
    "① <font color=brown>contraction最多只有一个固定的收敛点。序列: $x, f(x), f(f(x)), ...$会收敛到该点。</font> \\\n",
    "<font color=gray>简证：如果有两个的话，那么会向两个位置收敛，就无法满足distance缩小的条件了。</font>\\\n",
    "② <font color=brown>当$x=f(x)$时，就达到了收敛点。</font> \\\n",
    "· <font color=gray>eg: f(x)=x/2是contraction。当x=0时，收敛点是0，$x=0时，x=f(0)=0$。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7a5bd",
   "metadata": {},
   "source": [
    "2. **证明1：以max norm为distance measure时，bellman operator是一个contraction。**\\\n",
    "分析：\\\n",
    "① max norm：将$V$看做有$|S|$个元素的vector，则其max norm可以表示为：$$\\left \\| V \\right \\|_{\\infty} = \\underset{s\\in S}{max} |V(s)|$$\n",
    "② Bellman update:$$\\begin{align} \n",
    "& V_{k+1}(s) \\leftarrow \\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V_k(s^{'})]\\\\\n",
    "& 取B(V(s))=\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})]\\\\\n",
    "& B(V)=\\begin{pmatrix}\n",
    "B(V(s_1)) \\\\\n",
    "B(V(s_2)) \\\\\n",
    "... \\\\\n",
    "B(V(s_m))\n",
    "\\end{pmatrix}, s\\in S=\\{s_1, s_2, ..., s_m\\}  \\\\\n",
    "& B称为Bellman\\ operator \\\\\n",
    "\\end{align}$$\n",
    "③ 定义Bellman operator: B(V) \n",
    "$$\\begin{align} \n",
    "& 取B(V(s))=\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})], s\\in S=\\{s_1, s_2, ..., s_m\\} \\\\\n",
    "& B(V)=\\begin{pmatrix}\n",
    "B(V(s_1)) \\\\\n",
    "B(V(s_2)) \\\\\n",
    "... \\\\\n",
    "B(V(s_m))\n",
    "\\end{pmatrix} =\\begin{pmatrix}\n",
    "\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s_1,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})] \\\\\n",
    "\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s_2,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})] \\\\\n",
    "... \\\\\n",
    "\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s_m,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})]\n",
    "\\end{pmatrix}\n",
    "\\end{align}$$\n",
    "④ 需要证明：$$\\left \\| B(V)-B(\\tilde V) \\right \\|_{\\infty} \\le \\gamma \\left \\| V-\\tilde V \\right \\| _{\\infty}$$\n",
    "<font color=green>思路：$$\\begin{align} \n",
    "& \\because \\left \\| B(V)-B(\\tilde V) \\right \\|_{\\infty}=\\underset{s\\in S}{max} \\left | B(V(s))-B(\\tilde V(s)) \\right | \\\\\n",
    "& \\therefore 只要证明，对\\forall s都有\\left | B(V(s))-B(\\tilde V(s)) \\right |\\le \\gamma \\left \\| V-\\tilde V \\right \\| _{\\infty}即可 \\\\\n",
    "\\end{align}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b85d1",
   "metadata": {},
   "source": [
    "2. 证明1（续） \\\n",
    "⑤ 证明过程：\\\n",
    "利用任意不等式都有的性质：$$|max\\ f(x)-max\\ h(x)|\\le max|f(x)-h(x)|$$\n",
    "$$\\begin{align} \n",
    "& |BV(s)-B\\tilde V(s)| \\\\\n",
    "& =\\left|\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})]-\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma \\tilde V(s^{'})]\\right|\\\\\n",
    "& \\le \\underset{a\\in A}{max}\\left |\\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})]-\\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma \\tilde V(s^{'})]  \\right | \\\\\n",
    "& = \\underset{a\\in A}{max}\\left |\\gamma \\sum_{s^{'}}^{}T(s,a,s^{'})V(s^{'}) - \\gamma \\sum_{s^{'}}^{}T(s,a,s^{'})\\tilde V(s^{'})\\right | \\\\\n",
    "& = \\gamma*\\underset{a\\in A}{max}\\left |\\sum_{s^{'}}^{}T(s,a,s^{'})[V(s^{'})-\\tilde V(s^{'})]  \\right | \\\\\n",
    "& \\le \\gamma*\\underset{a\\in A}{max}\\left |\\sum_{s^{'}}^{}T(s,a,s^{'})\\underset {s^{'}}{max}|V(s^{'})-\\tilde V(s^{'})| \\right | \\\\\n",
    "& = \\gamma*\\underset {s^{'}}{max}\\left | V(s^{'})-\\tilde V(s^{'}) \\right |=\\gamma \\left \\| V-\\tilde V \\right \\|_{\\infty}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0680fe",
   "metadata": {},
   "source": [
    "3. **证明2：value iteration会收敛到唯一最优解** \\\n",
    "证明：\\\n",
    "Bellman equation迭代的过程相当于iterated sequence：$x, f(x), f(f(x)), ...$。\\\n",
    "根据contraction的性质，iterated sequence会收敛到固定值，且当$x=f(x)$时到达该收敛点。\\\n",
    "因此，$B(V)$会收敛到唯一的固定值：$$B(V)=\\begin{pmatrix}\n",
    "B(V(s_1)) \\\\\n",
    "B(V(s_2)) \\\\\n",
    "... \\\\\n",
    "B(V(s_m))\n",
    "\\end{pmatrix} \\rightarrow \\begin{pmatrix}\n",
    "V^*(s_1) \\\\\n",
    "V^*(s_2) \\\\\n",
    "... \\\\\n",
    "V^*(s_m)\n",
    "\\end{pmatrix}=V^* \\\\\n",
    "且此时，V^*=B(V^*)=\\begin{pmatrix}\n",
    "B(V^*(s_1)) \\\\\n",
    "B(V^*(s_2)) \\\\\n",
    "... \\\\\n",
    "B(V^*(s_m))\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a2717",
   "metadata": {},
   "source": [
    "#### III.1.3 收敛速度：rate of convergence\n",
    "1. <font color=blue>**value iteration converges exponentially fast.**</font> \\\n",
    "分析：$取error_k = \\left \\| B(V_{k})-B(V^*) \\right \\|_{\\infty}$\n",
    "$$\\begin{align} \n",
    "\\because \\left \\| B(V_{k+1})-B(V^*) \\right \\|_{\\infty} & \\le \\gamma *\\left \\| V_{k+1}-V^* \\right \\|_{\\infty} \\\\\n",
    "& = \\gamma *\\left \\| B(V_{k})-V^* \\right \\|_{\\infty} \n",
    "\\end{align}$$\n",
    "$$\\begin{align} \n",
    "\\therefore \\frac{\\left \\| B(V_{k+1})-B(V^*) \\right \\|_{\\infty}}{\\left \\| B(V_{k})-B(V^*) \\right \\|_{\\infty}}\\le \\gamma \\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "2. 要让收敛精度达到$\\epsilon$，则需要的迭代条件是$\\left \\| V_{k+1}-V_k \\right \\|_{\\infty} \\le \\epsilon (1-\\gamma)/\\gamma$。因为： \\\n",
    "$$if\\ \\left \\| V_{k+1}-V_k \\right \\|_{\\infty} \\le \\epsilon (1-\\gamma)/\\gamma,\\ \\ then\\ \\left \\| V_{k+1}-V^* \\right \\|_{\\infty} \\le \\epsilon$$\n",
    "分析：(略)<font color=blue>[详见AI：a modern approach 4th ch17.2 page575] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4de5e2",
   "metadata": {},
   "source": [
    "### III.2 policy iteration\n",
    "出发点：策略的收敛通常比value收敛发生的早很多。而bellman equation本身的目标就是找到最优策略。因此一个思路是直接求策略，而不是先求optimal value再得到对应的策略。\n",
    "#### III.1.1 算法\n",
    "1. **思路**: <font color=green>每次迭代先做policy evaluation，再做policy improvement，直到policy evaluation得到的utility没有变化，也即utility收敛到了contraction的收敛点。此时policy extraction得到的就是最优策略。</font>\\\n",
    "① <font color=blue>**policy evaluation**</font>：给定策略$\\pi_i$，对所有的states s计算value/utility:$$\\begin{align} \n",
    "& 解线性方程组：V^{\\pi_i}(s)=\\sum_{s^{'}}^{}T(s,\\pi_i(s),s^{'})[R(s,\\pi_i(s),s^{'})+\\gamma V^{\\pi_i}(s^{'})] \\\\\n",
    "& or\\\\\n",
    "& 迭代至收敛：V^{\\pi_i}(s)\\leftarrow \\sum_{s^{'}}^{}T(s,\\pi_i(s),s^{'})[R(s,\\pi_i(s),s^{'})+\\gamma V^{\\pi_i}(s^{'})]\\\\\n",
    "\\end{align}$$\n",
    "<font color=red>注：迭代法的收敛条件不应该表达为$$V^{\\pi_i}_{k+1}(s)\\leftarrow \\sum_{s^{'}}^{}T(s,\\pi_i(s),s^{'})[R(s,\\pi_i(s),s^{'})+\\gamma V^{\\pi_i}_{k}(s^{'})]$$\n",
    "因为下标k表示action的time horizon，它只是在迭代没有收敛的阶段标记time horizon。而在unlimited time horizon的环境下，一旦收敛之后，time horizon的变化已经不会带来utility大小的改变。</font> \\\n",
    "② <font color=blue>**policy improvement/extraction**</font>：向前看一步，更新策略$$\\begin{align} \n",
    "\\pi_{i+1}(s) & \\leftarrow \\underset{a\\in A}{argmax}Q^{\\pi_i}(s, a)\\\\\n",
    "& =\\underset{a\\in A}{argmax} \\sum_{s^{'}}P(s^{'}|s, a)[R(s, a, s^{'})+\\gamma V^{\\pi_i}(s^{'})] \n",
    "\\end{align}$$\n",
    "2. **Q-value iteration算法**： \\\n",
    "已知transition model $T(s,a,s^{'})$，rewards $R(s,a,s^{'})$，收敛精度$\\epsilon$，discount $\\gamma$ \\\n",
    "初始化：$V_0^{\\pi}(s)=0$，任意初始化$\\pi_0$ \\\n",
    "迭代: \\\n",
    "$$\\begin{align} \n",
    "repeat:&\\\\\n",
    "& \\#\\ policy\\ evaluation,解|S|个方程|S|个未知数的线性方程组 \\\\\n",
    "&for\\ s\\ in\\ S: \\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ \\ V^{\\pi}(s)=\\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,a,s^{'})+\\gamma V^{\\pi}(s^{'})] \\\\\n",
    "& \\#\\ policy\\ improvement/extraction \\\\\n",
    "& for\\ s\\ in\\ S: \\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ \\ \\pi_{i+1}(s) \\leftarrow\\underset{a\\in A}{argmax} \\sum_{s^{'}}P(s^{'}|s, a)[R(s, a, s^{'})+\\gamma V^{\\pi_i}(s^{'})]\\\\\n",
    "until:&\\  \\pi\\ stable\\\\\n",
    "return:&\\ \\pi\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407f830",
   "metadata": {},
   "source": [
    "3. 算法复杂度 \\\n",
    "· **policy evaluation** \\\n",
    "① 精确求解：直接求解线性方程组，那么正常情况下是$O(|S|^3)$。但如果利用状态转移函数的稀疏性，可以大幅降低复杂度。\\\n",
    "② 近似求解：用迭代法得到$V^{\\pi}$的近似估计值\n",
    "$$V^{\\pi}(s)\\leftarrow \\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,\\pi(s),s^{'})+\\gamma V^{\\pi}(s^{'})]$$\n",
    "单次迭代的复杂度是$O(|S|^2)$ \\\n",
    "· **policy improvement**: $O(|S|^2|A|)$ \\\n",
    "分析：每个for循环内部的一轮计算是$O(|S||A|)$，每次iteration要做$|S|$次for循环。所以每次iteration的复杂度是$O(|S|^2|A|)$ \\\n",
    "<font color=red>分析：直接看算法复杂度时，policy iteration的每次迭代复杂度是$O(|S|^2|A|)=O(|S|^2|A|)+O(|S|^2)$。从单次迭代的复杂度来看，它和value iteration在同一水平上。但policy iteration需要的总的迭代次数更少。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d425a",
   "metadata": {},
   "source": [
    "#### III.1.2 policy iteration能收敛到唯一最优解\n",
    "1. **每次policy improvement之后，新得到的policy一定比之前的policy有更好的value。**\n",
    "$$\n",
    "V^{\\pi^{'}}(s)\\ge V^{\\pi}(s), \\forall s\\in \\mathcal{S}\n",
    "$$\n",
    "证明[详见西湖书4.2.1 page73]\n",
    "2. **policy evaluation和policy improvement的迭代可以收敛到$V^{*}(s), \\forall s\\in \\mathcal{S}$。**\\\n",
    "证明思路：每一轮policy evaluation + policy improvement的迭代完成后得到的$V^{\\pi}(s)$比value iteration每次迭代后的值更大，更接近$V^{*}(s)$。而value iteration可以收敛，因此policy iteration也能收敛。[详见西湖书4.2.1 page75]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c24f6",
   "metadata": {},
   "source": [
    "3. **证明：迭代法做policy evaluation可以收敛，且会收敛到$V^{\\pi}(s)$的唯一解。** \\\n",
    "(1)value iteration of fixed policy对应的bellman operator是一个contraction。 \n",
    "$$V^{\\pi}(s)\\leftarrow \\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,\\pi(s),s^{'})+\\gamma V^{\\pi}(s^{'})]$$\n",
    "<font color=red>注，下面$BV(s)$是$BV^{\\pi}(s)$的简写，指固定策略后的value，其它同理。</font>\n",
    "$$\\begin{align} \n",
    "& |BV(s)-B\\tilde {V}(s)|， \\\\\n",
    "& = \\left |\\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,\\pi(s),s^{'})+\\gamma V(s^{'})]-\\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,\\pi(s),s^{'})+\\gamma \\tilde V(s^{'})]  \\right | \\\\\n",
    "& = \\left |\\gamma \\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})V(s^{'}) - \\gamma \\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})\\tilde V(s^{'})\\right | \\\\\n",
    "& = \\gamma*\\left |\\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[V(s^{'})-\\tilde V(s^{'})]  \\right | \\\\\n",
    "& \\le \\gamma*\\left |\\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})\\underset {s^{'}}{max}|V(s^{'})-\\tilde V(s^{'})| \\right | \\\\\n",
    "& = \\gamma*\\underset {s^{'}}{max}\\left | V(s^{'})-\\tilde V(s^{'}) \\right |   \\\\\n",
    "& =\\gamma \\left \\| V-\\tilde V \\right \\|_{\\infty}\n",
    "\\end{align} $$\n",
    "(2)证明：value iteration of fixed policy会收敛到唯一解。 \\\n",
    "直接用contraction性质。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa5251",
   "metadata": {},
   "source": [
    "### 附：对preference independent性质的说明\n",
    "1. 定义: Utility function的自变量称为attributes。有attributes X,Y。如果preferences for different levels of X do not depend on the level of Y, and vice versa就称他们是preference independent的。 \n",
    "2. 数学表达: 如果X和Y是preference independent的, the utility function U(X,Y)可以写成U(X,Y) = f(X) + g(Y)。\n",
    "3. 例子：如果改变Y的大小会影响X的utility，那么就不满足preference independent条件。比如当苹果的拥有量很高的时候，拥有更多香蕉的效用会下降，因为他们都是甜水果。但是当苹果拥有量很多的时候，不会影响拥有汽车的效用。此时苹果和汽车是preference independent的，但苹果和香蕉不是。\n",
    "4. 应用说明：在MDP设定下，preference independent按含在markov假设中。每个time step上的states就是attributes。$R_t$只取决于$S_t$，不受其他时点上的states影响。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
